[
  {
    "objectID": "slides/1_model_prompt_parser.html#python",
    "href": "slides/1_model_prompt_parser.html#python",
    "title": "Models, Prompts and Output Parsers",
    "section": "Python",
    "text": "Python\n\nfrom langchain.output_parsers import StructuredOutputParser\nfrom langchain.output_parsers import ResponseSchema\nfrom langchain.prompts import ChatPromptTemplate\nfrom langchain.chat_models import ChatOpenAI\nimport datetime\nimport os\nimport openai\n\nfrom dotenv import load_dotenv, find_dotenv\n_ = load_dotenv(find_dotenv())  # read local .env file\nopenai.api_key = os.environ['OPENAI_API_KEY']"
  },
  {
    "objectID": "slides/1_model_prompt_parser.html#helper-function-get_completion",
    "href": "slides/1_model_prompt_parser.html#helper-function-get_completion",
    "title": "Models, Prompts and Output Parsers",
    "section": "Helper function: get_completion",
    "text": "Helper function: get_completion\n\nLet’s start with a direct API calls to OpenAI.\n\n\n\nllm_model = \"gpt-3.5-turbo\"\n\n\ndef get_completion(prompt, model=llm_model):\n    messages = [{\"role\": \"user\", \"content\": prompt}]\n    response = openai.ChatCompletion.create(\n        model=model,\n        messages=messages,\n        temperature=0,\n    )\n    return response.choices[0].message[\"content\"]"
  },
  {
    "objectID": "slides/1_model_prompt_parser.html#example",
    "href": "slides/1_model_prompt_parser.html#example",
    "title": "Models, Prompts and Output Parsers",
    "section": "Example",
    "text": "Example\n\nget_completion(\"What is 1+1?\")\n\n\n‘1+1 equals 2.’"
  },
  {
    "objectID": "slides/1_model_prompt_parser.html#customer-email",
    "href": "slides/1_model_prompt_parser.html#customer-email",
    "title": "Models, Prompts and Output Parsers",
    "section": "Customer Email",
    "text": "Customer Email\n\ncustomer_email = \"\"\"\nArrr, I be fuming that me blender lid \\\nflew off and splattered me kitchen walls \\\nwith smoothie! And to make matters worse,\\\nthe warranty don't cover the cost of \\\ncleaning up me kitchen. I need yer help \\\nright now, matey!\n\"\"\""
  },
  {
    "objectID": "slides/1_model_prompt_parser.html#prompt",
    "href": "slides/1_model_prompt_parser.html#prompt",
    "title": "Models, Prompts and Output Parsers",
    "section": "Prompt",
    "text": "Prompt\n\nstyle = \"\"\"American English \\\nin a calm and respectful tone\n\"\"\"\n\n\n\nprompt = f\"\"\"Translate the text \\\nthat is delimited by triple backticks \ninto a style that is {style}.\ntext: ```{customer_email}```\n\"\"\""
  },
  {
    "objectID": "slides/1_model_prompt_parser.html#result",
    "href": "slides/1_model_prompt_parser.html#result",
    "title": "Models, Prompts and Output Parsers",
    "section": "Result",
    "text": "Result\n\nresponse = get_completion(prompt)\n\n\n\nresponse\n\n-’ I am quite frustrated that my blender lid flew off and made a mess of my kitchen walls with smoothie! To add to my frustration, the warranty does not cover the cost of cleaning up my kitchen. I kindly request your assistance at this moment, my friend.’"
  },
  {
    "objectID": "slides/1_model_prompt_parser.html#model",
    "href": "slides/1_model_prompt_parser.html#model",
    "title": "Models, Prompts and Output Parsers",
    "section": "Model",
    "text": "Model\n\n# To control the randomness and creativity of the generated\n# text by an LLM, use temperature = 0.0\nchat = ChatOpenAI(temperature=0.0, model=llm_model)\nchat\n\n\nChatOpenAI(cache=None, verbose=False, callbacks=None, callback_manager=None, tags=None, metadata=None, client=&lt;class ‘openai.api_resources.chat_completion.ChatCompletion’&gt;, model_name=‘gpt-3.5-turbo’, temperature=0.0, model_kwargs={}, openai_api_key=‘xxx’, openai_api_base=’‘, openai_organization=’xxx’, openai_proxy=’’, request_timeout=None, max_retries=6, streaming=False, n=1, max_tokens=None, tiktoken_model_name=None)"
  },
  {
    "objectID": "slides/1_model_prompt_parser.html#define-prompt-template",
    "href": "slides/1_model_prompt_parser.html#define-prompt-template",
    "title": "Models, Prompts and Output Parsers",
    "section": "Define prompt template",
    "text": "Define prompt template\n\ntemplate_string = \"\"\"Translate the text \\\nthat is delimited by triple backticks \\\ninto a style that is {style}. \\\ntext: ```{text}```\n\"\"\"\n\n\n\nprompt_template = ChatPromptTemplate.from_template(template_string)"
  },
  {
    "objectID": "slides/1_model_prompt_parser.html#inspect-prompt-template",
    "href": "slides/1_model_prompt_parser.html#inspect-prompt-template",
    "title": "Models, Prompts and Output Parsers",
    "section": "Inspect prompt template",
    "text": "Inspect prompt template\n\nprompt_template.messages[0].prompt\n\n\nPromptTemplate(input_variables=[‘style’, ‘text’], output_parser=None, partial_variables={}, template=‘Translate the text that is delimited by triple backticks into a style that is {style}. text: {text}’, template_format=‘f-string’, validate_template=True)\n\n\n\nprompt_template.messages[0].prompt.input_variables\n\n\n[‘style’, ‘text’]"
  },
  {
    "objectID": "slides/1_model_prompt_parser.html#customer-template-input",
    "href": "slides/1_model_prompt_parser.html#customer-template-input",
    "title": "Models, Prompts and Output Parsers",
    "section": "Customer template input",
    "text": "Customer template input\n\nText:\n\n\n\ncustomer_email = \"\"\"\nArrr, I be fuming that me blender lid \\\nflew off and splattered me kitchen walls \\\nwith smoothie! And to make matters worse, \\\nthe warranty don't cover the cost of \\\ncleaning up me kitchen. I need yer help \\\nright now, matey!\n\"\"\"\n\n\nStyle:\n\n\n\n\ncustomer_style = \"\"\"American English \\\nin a calm and respectful tone\n\"\"\""
  },
  {
    "objectID": "slides/1_model_prompt_parser.html#create-template",
    "href": "slides/1_model_prompt_parser.html#create-template",
    "title": "Models, Prompts and Output Parsers",
    "section": "Create template",
    "text": "Create template\n\ncustomer_messages = prompt_template.format_messages(\n    style=customer_style,\n    text=customer_email)\n\n\n\nprint(type(customer_messages))\nprint(type(customer_messages[0]))\n\n\n&lt;class ‘list’&gt;\n&lt;class ‘langchain.schema.messages.HumanMessage’&gt;"
  },
  {
    "objectID": "slides/1_model_prompt_parser.html#customer-messages-prompt",
    "href": "slides/1_model_prompt_parser.html#customer-messages-prompt",
    "title": "Models, Prompts and Output Parsers",
    "section": "Customer messages prompt",
    "text": "Customer messages prompt\n\nprint(customer_messages[0])\n\n\ncontent=“Translate the text that is delimited by triple backticks into a style that is American English in a calm and respectful tone. text: \\nArrr, I be fuming that me blender lid flew off and splattered me kitchen walls with smoothie! And to make matters worse, the warranty don't cover the cost of cleaning up me kitchen. I need yer help right now, matey!\\n” additional_kwargs={} example=False"
  },
  {
    "objectID": "slides/1_model_prompt_parser.html#customer-messages-response",
    "href": "slides/1_model_prompt_parser.html#customer-messages-response",
    "title": "Models, Prompts and Output Parsers",
    "section": "Customer messages response",
    "text": "Customer messages response\n\n# Call the LLM to translate to the style of the customer message\ncustomer_response = chat(customer_messages)\n\n\n\nprint(customer_response.content)\n\n\nI’m really frustrated that my blender lid flew off and made a mess of my kitchen walls with smoothie! And to make things even worse, the warranty doesn’t cover the cost of cleaning up my kitchen. I could really use your help right now, my friend!"
  },
  {
    "objectID": "slides/1_model_prompt_parser.html#service-reply-input",
    "href": "slides/1_model_prompt_parser.html#service-reply-input",
    "title": "Models, Prompts and Output Parsers",
    "section": "Service reply input",
    "text": "Service reply input\n\nservice_reply = \"\"\"Hey there customer, \\\nthe warranty does not cover \\\ncleaning expenses for your kitchen \\\nbecause it's your fault that \\\nyou misused your blender \\\nby forgetting to put the lid on before \\\nstarting the blender. \\\nTough luck! See ya!\n\"\"\"\n\n\n\nservice_style_pirate = \"\"\"\\\na polite tone \\\nthat speaks in English Pirate\\\n\"\"\""
  },
  {
    "objectID": "slides/1_model_prompt_parser.html#service-reply-prompt-template",
    "href": "slides/1_model_prompt_parser.html#service-reply-prompt-template",
    "title": "Models, Prompts and Output Parsers",
    "section": "Service reply prompt template",
    "text": "Service reply prompt template\n\nservice_messages = prompt_template.format_messages(\n    style=service_style_pirate,\n    text=service_reply)\n\nprint(service_messages[0].content)\n\n\nTranslate the text that is delimited by triple backticks into a style that is a polite tone that speaks in English Pirate. text: ```Hey there customer, the warranty does not cover cleaning expenses for your kitchen because it’s your fault that you misused your blender by forgetting to put the lid on before starting the blender. Tough luck! See ya! ```"
  },
  {
    "objectID": "slides/1_model_prompt_parser.html#service-reply-response",
    "href": "slides/1_model_prompt_parser.html#service-reply-response",
    "title": "Models, Prompts and Output Parsers",
    "section": "Service reply response",
    "text": "Service reply response\n\nservice_response = chat(service_messages)\nprint(service_response.content)\n\n\nAhoy there, matey! I regret to inform ye that the warranty be not coverin’ the costs o’ cleanin’ yer galley, as ‘tis yer own fault fer misusin’ yer blender by forgettin’ to secure the lid afore startin’ it. Aye, tough luck, me heartie! Fare thee well!"
  },
  {
    "objectID": "slides/1_model_prompt_parser.html#output-style",
    "href": "slides/1_model_prompt_parser.html#output-style",
    "title": "Models, Prompts and Output Parsers",
    "section": "Output style",
    "text": "Output style\n\nLet’s start with defining how we would like the LLM output to look like:\n\n\n\n{\n    \"gift\": False,\n    \"delivery_days\": 5,\n    \"price_value\": \"pretty affordable!\"\n}"
  },
  {
    "objectID": "slides/1_model_prompt_parser.html#customer-review-and-template",
    "href": "slides/1_model_prompt_parser.html#customer-review-and-template",
    "title": "Models, Prompts and Output Parsers",
    "section": "Customer review and template",
    "text": "Customer review and template\n\ncustomer_review = \"\"\"\\\nThis leaf blower is pretty amazing.  It has four settings:\\\ncandle blower, gentle breeze, windy city, and tornado. \\\nIt arrived in two days, just in time for my wife's \\\nanniversary present. \\\nI think my wife liked it so much she was speechless. \\\nSo far I've been the only one using it, and I've been \\\nusing it every other morning to clear the leaves on our lawn. \\\nIt's slightly more expensive than the other leaf blowers \\\nout there, but I think it's worth it for the extra features.\n\"\"\"\n\nreview_template = \"\"\"\\\nFor the following text, extract the following information:\n\ngift: Was the item purchased as a gift for someone else? \\\nAnswer True if yes, False if not or unknown.\n\ndelivery_days: How many days did it take for the product \\\nto arrive? If this information is not found, output -1.\n\nprice_value: Extract any sentences about the value or price,\\\nand output them as a comma separated Python list.\n\nFormat the output as JSON with the following keys:\ngift\ndelivery_days\nprice_value\n\ntext: {text}\n\"\"\""
  },
  {
    "objectID": "slides/1_model_prompt_parser.html#prompt-template",
    "href": "slides/1_model_prompt_parser.html#prompt-template",
    "title": "Models, Prompts and Output Parsers",
    "section": "Prompt template",
    "text": "Prompt template\n\nprompt_template = ChatPromptTemplate.from_template(review_template)\nprint(prompt_template)\n\n\ninput_variables=[‘text’] output_parser=None partial_variables={} messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=[‘text’], output_parser=None, partial_variables={}, template=’For the following text, extract the following information:: Was the item purchased as a gift for someone else? Answer True if yes, False if not or unknown._days: How many days did it take for the product to arrive? If this information is not found, output -1._value: Extract any sentences about the value or price,and output them as a comma separated Python list.the output as JSON with the following keys:_days_value: {text}‘, template_format=’f-string’, validate_template=True), additional_kwargs={})]"
  },
  {
    "objectID": "slides/1_model_prompt_parser.html#response",
    "href": "slides/1_model_prompt_parser.html#response",
    "title": "Models, Prompts and Output Parsers",
    "section": "Response",
    "text": "Response\n\nmessages = prompt_template.format_messages(text=customer_review)\nchat = ChatOpenAI(temperature=0.0, model=llm_model)\nresponse = chat(messages)\nprint(response.content)\n\n\n{\n  \"gift\": false,\n  \"delivery_days\": 2,\n  \"price_value\": [\"It's slightly more expensive than the other leaf blowers out there, but I think it's worth it for the extra features.\"],\n  \"text\": \"This leaf blower is pretty amazing. It has four settings:candle blower, gentle breeze, windy city, and tornado. It arrived in two days, just in time for my wife's anniversary present. I think my wife liked it so much she was speechless. So far I've been the only one using it, and I've been using it every other morning to clear the leaves on our lawn. It's slightly more expensive than the other leaf blowers out there, but I think it's worth it for the extra features.\"\n}"
  },
  {
    "objectID": "slides/1_model_prompt_parser.html#inspect-response",
    "href": "slides/1_model_prompt_parser.html#inspect-response",
    "title": "Models, Prompts and Output Parsers",
    "section": "Inspect response",
    "text": "Inspect response\n\ntype(response.content)\n\n\nstr\n\n\n\n# You will get an error by running this line of code\n# because'gift' is not a dictionary\n# 'gift' is a string\nresponse.content.get('gift')\n\n\n---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\n/Users/jankirenz/labs/lab-langchain-basics/slides/1_model_prompt_parser.qmd in line 1\n----&gt; 342 response.content.get('gift')\n\nAttributeError: 'str' object has no attribute 'get'"
  },
  {
    "objectID": "slides/1_model_prompt_parser.html#parse-output-into-dictionary-1",
    "href": "slides/1_model_prompt_parser.html#parse-output-into-dictionary-1",
    "title": "Models, Prompts and Output Parsers",
    "section": "Parse output into dictionary",
    "text": "Parse output into dictionary\n\nParse the LLM output string into a Python dictionary\n\n\n\ngift_schema = ResponseSchema(name=\"gift\",\n                             description=\"Was the item purchased\\\n                             as a gift for someone else? \\\n                             Answer True if yes,\\\n                             False if not or unknown.\")\n\ndelivery_days_schema = ResponseSchema(name=\"delivery_days\",\n                                      description=\"How many days\\\n                                      did it take for the product\\\n                                      to arrive? If this \\\n                                      information is not found,\\\n                                      output -1.\")\n\nprice_value_schema = ResponseSchema(name=\"price_value\",\n                                    description=\"Extract any\\\n                                    sentences about the value or \\\n                                    price, and output them as a \\\n                                    comma separated Python list.\")\n\nresponse_schemas = [gift_schema,\n                    delivery_days_schema,\n                    price_value_schema]"
  },
  {
    "objectID": "slides/1_model_prompt_parser.html#structuredoutputparser",
    "href": "slides/1_model_prompt_parser.html#structuredoutputparser",
    "title": "Models, Prompts and Output Parsers",
    "section": "StructuredOutputParser",
    "text": "StructuredOutputParser\n\noutput_parser = StructuredOutputParser.from_response_schemas(response_schemas)\n\n\n\nformat_instructions = output_parser.get_format_instructions()\n\n\nprint(format_instructions)\n\n\nThe output should be a markdown code snippet formatted in the following schema, including the leading and trailing “json\" and \"”:\n\n{\n    \"gift\": string  // Was the item purchased                             as a gift for someone else?                              Answer True if yes,                             False if not or unknown.\n    \"delivery_days\": string  // How many days                                      did it take for the product                                      to arrive? If this                                       information is not found,                                      output -1.\n    \"price_value\": string  // Extract any                                    sentences about the value or                                     price, and output them as a                                     comma separated Python list.\n}"
  },
  {
    "objectID": "slides/1_model_prompt_parser.html#review-template-2",
    "href": "slides/1_model_prompt_parser.html#review-template-2",
    "title": "Models, Prompts and Output Parsers",
    "section": "Review template 2",
    "text": "Review template 2\n\nreview_template_2 = \"\"\"\\\nFor the following text, extract the following information:\n\ngift: Was the item purchased as a gift for someone else? \\\nAnswer True if yes, False if not or unknown.\n\ndelivery_days: How many days did it take for the product\\\nto arrive? If this information is not found, output -1.\n\nprice_value: Extract any sentences about the value or price,\\\nand output them as a comma separated Python list.\n\ntext: {text}\n\n{format_instructions}\n\"\"\"\n\nprompt = ChatPromptTemplate.from_template(template=review_template_2)\n\nmessages = prompt.format_messages(text=customer_review,\n                                  format_instructions=format_instructions)"
  },
  {
    "objectID": "slides/1_model_prompt_parser.html#inspect-template",
    "href": "slides/1_model_prompt_parser.html#inspect-template",
    "title": "Models, Prompts and Output Parsers",
    "section": "Inspect template",
    "text": "Inspect template\n\nprint(messages[0].content)\n\n\nFor the following text, extract the following information: gift: Was the item purchased as a gift for someone else? Answer True if yes, False if not or unknown. delivery_days: How many days did it take for the productto arrive? If this information is not found, output -1. price_value: Extract any sentences about the value or price,and output them as a comma separated Python list. text: This leaf blower is pretty amazing. It has four settings:candle blower, gentle breeze, windy city, and tornado. It arrived in two days, just in time for my wife’s anniversary present. I think my wife liked it so much she was speechless. So far I’ve been the only one using it, and I’ve been using it every other morning to clear the leaves on our lawn. It’s slightly more expensive than the other leaf blowers out there, but I think it’s worth it for the extra features.\n\nThe output should be a markdown code snippet formatted in the following schema, including the leading and trailing “json\" and \"”:\n{\n    \"gift\": string  // Was the item purchased                             as a gift for someone else?                              Answer True if yes,                             False if not or unknown.\n    \"delivery_days\": string  // How many days                                      did it take for the product                                      to arrive? If this                                       information is not found,                                      output -1.\n    \"price_value\": string  // Extract any                                    sentences about the value or                                     price, and output them as a                                     comma separated Python list.\n}"
  },
  {
    "objectID": "slides/1_model_prompt_parser.html#response-1",
    "href": "slides/1_model_prompt_parser.html#response-1",
    "title": "Models, Prompts and Output Parsers",
    "section": "Response",
    "text": "Response\n\nresponse = chat(messages)\n\n\n\nprint(response.content)\n\n\n\n{\n    \"gift\": false,\n    \"delivery_days\": \"2\",\n    \"price_value\": \"It's slightly more expensive than the other leaf blowers out there, but I think it's worth it for the extra features.\"\n}"
  },
  {
    "objectID": "slides/1_model_prompt_parser.html#parse-output",
    "href": "slides/1_model_prompt_parser.html#parse-output",
    "title": "Models, Prompts and Output Parsers",
    "section": "Parse output",
    "text": "Parse output\n\noutput_dict = output_parser.parse(response.content)\n\n\n\noutput_dict\n\n\n\n{'gift': False,\n 'delivery_days': '2',\n 'price_value': \"It's slightly more expensive than the other leaf blowers out there, but I think it's worth it for the extra features.\"}\n\ntype(output_dict)\n\n\ndict\n\n\noutput_dict.get('delivery_days')\n\n\n2"
  },
  {
    "objectID": "slides/2_memory.html#python",
    "href": "slides/2_memory.html#python",
    "title": "Memory",
    "section": "Python",
    "text": "Python\n\nfrom langchain.memory import ConversationSummaryBufferMemory\nfrom langchain.llms import OpenAI\nfrom langchain.memory import ConversationTokenBufferMemory\nfrom langchain.memory import ConversationBufferWindowMemory\nfrom langchain.memory import ConversationBufferMemory\nfrom langchain.chains import ConversationChain\nfrom langchain.chat_models import ChatOpenAI\nimport datetime\nimport warnings\nimport os\n\nfrom dotenv import load_dotenv, find_dotenv\n_ = load_dotenv(find_dotenv())  # read local .env file\n\nwarnings.filterwarnings('ignore')"
  },
  {
    "objectID": "slides/2_memory.html#setup-1",
    "href": "slides/2_memory.html#setup-1",
    "title": "Memory",
    "section": "Setup",
    "text": "Setup\n\nllm_model = \"gpt-3.5-turbo\"\n\nllm = ChatOpenAI(temperature=0.0, model=llm_model)\nmemory = ConversationBufferMemory()\n\nconversation = ConversationChain(\n    llm=llm,\n    memory=memory,\n    verbose=True  # shows conversation history\n)"
  },
  {
    "objectID": "slides/2_memory.html#some-examples",
    "href": "slides/2_memory.html#some-examples",
    "title": "Memory",
    "section": "Some examples",
    "text": "Some examples\n\nconversation.predict(input=\"Hi, my name is Jan\")\n\n\n“Hello Jan! It’s nice to meet you. How can I assist you today?”\n\n\nconversation.predict(input=\"What is 1+1?\")\n\n\n‘1+1 is equal to 2.’\n\n\nconversation.predict(input=\"What is my name?\")\n\n\n‘Your name is Jan.’"
  },
  {
    "objectID": "slides/2_memory.html#memory-buffer",
    "href": "slides/2_memory.html#memory-buffer",
    "title": "Memory",
    "section": "Memory buffer",
    "text": "Memory buffer\n\nprint(memory.buffer)\n\n\nHuman: Hi, my name is Jan\nAI: Hello Jan! It's nice to meet you. How can I assist you today?\nHuman: What is 1+1?\nAI: 1+1 is equal to 2.\nHuman: What is my name?\nAI: Your name is Jan.\n\n\n\nmemory.load_memory_variables({})\n\n\n{‘history’: “Human: Hi, my name is Jan: Hello Jan! It’s nice to meet you. How can I assist you today?: What is 1+1?: 1+1 is equal to 2.: What is my name?: Your name is Jan.”}"
  },
  {
    "objectID": "slides/2_memory.html#conversational-example",
    "href": "slides/2_memory.html#conversational-example",
    "title": "Memory",
    "section": "Conversational example",
    "text": "Conversational example\n\nmemory = ConversationBufferMemory()\n\n\nmemory.save_context({\"input\": \"Hi\"},\n                    {\"output\": \"What's up\"})\n\n\n\nprint(memory.buffer)\n\n\n\nHuman: Hi\nAI: What's up\n\n\n\nmemory.load_memory_variables({})\n\n\n{‘history’: “Human: Hi: What’s up”}"
  },
  {
    "objectID": "slides/2_memory.html#conversational-example-continued",
    "href": "slides/2_memory.html#conversational-example-continued",
    "title": "Memory",
    "section": "Conversational example continued",
    "text": "Conversational example continued\n\nmemory.save_context({\"input\": \"Not much, just hanging\"},\n                    {\"output\": \"Cool\"})\n\n\n\nmemory.load_memory_variables({})\n\n\n{‘history’: “Human: Hi: What’s up: Not much, just hanging: Cool”}"
  },
  {
    "objectID": "slides/2_memory.html#conversation-buffer-window-memory-1",
    "href": "slides/2_memory.html#conversation-buffer-window-memory-1",
    "title": "Memory",
    "section": "Conversation Buffer Window Memory",
    "text": "Conversation Buffer Window Memory\n\nmemory = ConversationBufferWindowMemory(k=1)"
  },
  {
    "objectID": "slides/2_memory.html#example",
    "href": "slides/2_memory.html#example",
    "title": "Memory",
    "section": "Example",
    "text": "Example\n\nmemory.save_context({\"input\": \"Hi\"},\n                    {\"output\": \"What's up\"})\n\nmemory.save_context({\"input\": \"Not much, just hanging\"},\n                    {\"output\": \"Cool\"})\n\n\n\nmemory.load_memory_variables({})\n\n\n{‘history’: ‘Human: Not much, just hanging: Cool’}"
  },
  {
    "objectID": "slides/2_memory.html#setup-2",
    "href": "slides/2_memory.html#setup-2",
    "title": "Memory",
    "section": "Setup",
    "text": "Setup\n\nllm = ChatOpenAI(temperature=0.0, model=llm_model)\n\nmemory = ConversationBufferWindowMemory(k=1)\n\nconversation = ConversationChain(\n    llm=llm,\n    memory=memory,\n    verbose=False\n)"
  },
  {
    "objectID": "slides/2_memory.html#example-1",
    "href": "slides/2_memory.html#example-1",
    "title": "Memory",
    "section": "Example",
    "text": "Example\n\nconversation.predict(input=\"Hi, my name is Jan\")\n\n\n“Hello Jan! It’s nice to meet you. How can I assist you today?”\n\n\n\nconversation.predict(input=\"What is 1+1?\")\n\n\n‘1+1 is equal to 2.’\n\n\n\n\nconversation.predict(input=\"What is my name?\")\n\n\n“I’m sorry, but I don’t have access to personal information about individuals unless it has been shared with me in the course of our conversation.”"
  },
  {
    "objectID": "slides/2_memory.html#setup-3",
    "href": "slides/2_memory.html#setup-3",
    "title": "Memory",
    "section": "Setup",
    "text": "Setup\n\nllm = ChatOpenAI(temperature=0.0, model=llm_model)\n\n\nmemory = ConversationTokenBufferMemory(llm=llm, max_token_limit=50)"
  },
  {
    "objectID": "slides/2_memory.html#example-2",
    "href": "slides/2_memory.html#example-2",
    "title": "Memory",
    "section": "Example",
    "text": "Example\n\nmemory.save_context({\"input\": \"AI is what?!\"},\n                    {\"output\": \"Amazing!\"})\n\nmemory.save_context({\"input\": \"Backpropagation is what?\"},\n                    {\"output\": \"Beautiful!\"})\nmemory.save_context({\"input\": \"Chatbots are what?\"},\n                    {\"output\": \"Charming!\"})\n\n\n\nmemory.load_memory_variables({})\n\n\n{‘history’: ‘AI: Amazing!: Backpropagation is what?: Beautiful!: Chatbots are what?: Charming!’}"
  },
  {
    "objectID": "slides/2_memory.html#example-3",
    "href": "slides/2_memory.html#example-3",
    "title": "Memory",
    "section": "Example",
    "text": "Example\n\n# create a long string\nschedule = \"There is a meeting at 8am with your product team. \\\nYou will need your powerpoint presentation prepared. \\\n9am-12pm have time to work on your LangChain \\\nproject which will go quickly because Langchain is such a powerful tool. \\\nAt Noon, lunch at the italian resturant with a customer who is driving \\\nfrom over an hour away to meet you to understand the latest in AI. \\\nBe sure to bring your laptop to show the latest LLM demo.\""
  },
  {
    "objectID": "slides/2_memory.html#setup-4",
    "href": "slides/2_memory.html#setup-4",
    "title": "Memory",
    "section": "Setup",
    "text": "Setup\n\nmemory = ConversationSummaryBufferMemory(llm=llm, max_token_limit=100)"
  },
  {
    "objectID": "slides/2_memory.html#example-4",
    "href": "slides/2_memory.html#example-4",
    "title": "Memory",
    "section": "Example",
    "text": "Example\n\nmemory.save_context({\"input\": \"Hello\"}, {\"output\": \"What's up\"})\nmemory.save_context({\"input\": \"Not much, just hanging\"},\n                    {\"output\": \"Cool\"})\nmemory.save_context({\"input\": \"What is on the schedule today?\"},\n                    {\"output\": f\"{schedule}\"})"
  },
  {
    "objectID": "slides/2_memory.html#history",
    "href": "slides/2_memory.html#history",
    "title": "Memory",
    "section": "History",
    "text": "History\n\nmemory.load_memory_variables({})\n\n\n{‘history’: ‘System: The human and AI exchange greetings. The human asks about the schedule for the day. The AI provides a detailed schedule, including a meeting with the product team, work on the LangChain project, and a lunch meeting with a customer interested in AI. The AI emphasizes the importance of bringing a laptop to showcase the latest LLM demo during the lunch meeting.’}"
  },
  {
    "objectID": "slides/2_memory.html#conversationchain",
    "href": "slides/2_memory.html#conversationchain",
    "title": "Memory",
    "section": "ConversationChain",
    "text": "ConversationChain\n\nconversation = ConversationChain(\n    llm=llm,\n    memory=memory,\n    verbose=True\n)"
  },
  {
    "objectID": "slides/2_memory.html#conversation-predict",
    "href": "slides/2_memory.html#conversation-predict",
    "title": "Memory",
    "section": "Conversation predict",
    "text": "Conversation predict\n\nconversation.predict(input=\"What would be a good demo to show?\")\n\n\n“A good demo to show during the lunch meeting with the customer interested in AI would be the latest LLM (Language Model) demo. The LLM is a cutting-edge AI model that can generate human-like text based on a given prompt. It has been trained on a vast amount of data and can generate coherent and contextually relevant responses. By showcasing the LLM demo, you can demonstrate the capabilities of AI in natural language processing and generate interest in potential applications for the customer’s business.”"
  },
  {
    "objectID": "slides/2_memory.html#memory-1",
    "href": "slides/2_memory.html#memory-1",
    "title": "Memory",
    "section": "Memory",
    "text": "Memory\n\nmemory.load_memory_variables({})\n\n\n{‘history’: “System: The human and AI exchange greetings and discuss the schedule for the day. The AI provides a detailed schedule, including a meeting with the product team, work on the LangChain project, and a lunch meeting with a customer interested in AI. The AI emphasizes the importance of bringing a laptop to showcase the latest LLM demo during the lunch meeting. The human asks what would be a good demo to show, and the AI suggests showcasing the latest LLM (Language Model) demo. The LLM is a cutting-edge AI model that can generate human-like text based on a given prompt. By showcasing the LLM demo, the AI can demonstrate the capabilities of AI in natural language processing and generate interest in potential applications for the customer’s business.”}"
  },
  {
    "objectID": "slides/3_chains.html#python",
    "href": "slides/3_chains.html#python",
    "title": "Chains",
    "section": "Python",
    "text": "Python\n\nfrom langchain.chains.router.llm_router import LLMRouterChain, RouterOutputParser\nfrom langchain.prompts import PromptTemplate\nfrom langchain.chains.router import MultiPromptChain\nfrom langchain.chains import SequentialChain\nfrom langchain.chains import SimpleSequentialChain\nfrom langchain.chains import LLMChain\nfrom langchain.prompts import ChatPromptTemplate\nfrom langchain.chat_models import ChatOpenAI\nfrom dotenv import load_dotenv, find_dotenv\nimport os\nimport pandas as pd\nimport warnings\nwarnings.filterwarnings('ignore')\n\n_ = load_dotenv(find_dotenv())  # read local .env file"
  },
  {
    "objectID": "slides/3_chains.html#data",
    "href": "slides/3_chains.html#data",
    "title": "Chains",
    "section": "Data",
    "text": "Data\n\ndf = pd.read_csv('../data/Data.csv')\n\n\ndf.head()"
  },
  {
    "objectID": "slides/3_chains.html#llmchain",
    "href": "slides/3_chains.html#llmchain",
    "title": "Chains",
    "section": "LLMChain",
    "text": "LLMChain\n\nInitialize model\n\n\n\nllm_model = \"gpt-3.5-turbo\"\n\nllm = ChatOpenAI(temperature=0.9, model=llm_model)\n\n\nInitialize prompt\n\n\n\n\nprompt = ChatPromptTemplate.from_template(\n    \"What is the best name to describe \\\n    a company that makes {product}?\"\n)\n\n\n\n\nCombine to chain\n\n\nchain = LLMChain(llm=llm, prompt=prompt)"
  },
  {
    "objectID": "slides/3_chains.html#example",
    "href": "slides/3_chains.html#example",
    "title": "Chains",
    "section": "Example",
    "text": "Example\n\nproduct = \"Queen Size Sheet Set\"\n\nchain.run(product)\n\n\n‘“Royal Rest Linens”’"
  },
  {
    "objectID": "slides/3_chains.html#simplesequentialchain",
    "href": "slides/3_chains.html#simplesequentialchain",
    "title": "Chains",
    "section": "SimpleSequentialChain",
    "text": "SimpleSequentialChain\n\nllm = ChatOpenAI(temperature=0.9, model=llm_model)\n\n# prompt template 1\nfirst_prompt = ChatPromptTemplate.from_template(\n    \"What is the best name to describe \\\n    a company that makes {product}?\"\n)\n\n# Chain 1\nchain_one = LLMChain(llm=llm, prompt=first_prompt)\n\n\n\n# prompt template 2\nsecond_prompt = ChatPromptTemplate.from_template(\n    \"Write a 20 words description for the following \\\n    company:{company_name}\"\n)\n# chain 2\nchain_two = LLMChain(llm=llm, prompt=second_prompt)"
  },
  {
    "objectID": "slides/3_chains.html#overall-simple-chain",
    "href": "slides/3_chains.html#overall-simple-chain",
    "title": "Chains",
    "section": "Overall simple chain",
    "text": "Overall simple chain\n\noverall_simple_chain = SimpleSequentialChain(chains=[chain_one, chain_two],\n                                             verbose=True\n                                             )\n\n\n\noverall_simple_chain.run(product)\n\n\n‘Regal Rest Linens offers high-quality, luxurious linens for a comfortable and rejuvenating sleep experience. Perfect for a regal touch.’"
  },
  {
    "objectID": "slides/3_chains.html#sequentialchain",
    "href": "slides/3_chains.html#sequentialchain",
    "title": "Chains",
    "section": "SequentialChain",
    "text": "SequentialChain\n\nllm = ChatOpenAI(temperature=0.9, model=llm_model)\n\n# prompt template 1: translate to english\nfirst_prompt = ChatPromptTemplate.from_template(\n    \"Translate the following review to english:\"\n    \"\\n\\n{Review}\"\n)\n# chain 1: input= Review and output= English_Review\nchain_one = LLMChain(llm=llm, prompt=first_prompt,\n                     output_key=\"English_Review\"\n                     )\n\n\n\nsecond_prompt = ChatPromptTemplate.from_template(\n    \"Can you summarize the following review in 1 sentence:\"\n    \"\\n\\n{English_Review}\"\n)\n# chain 2: input= English_Review and output= summary\nchain_two = LLMChain(llm=llm, prompt=second_prompt,\n                     output_key=\"summary\"\n                     )"
  },
  {
    "objectID": "slides/3_chains.html#section",
    "href": "slides/3_chains.html#section",
    "title": "Chains",
    "section": "",
    "text": "# prompt template 3: translate to english\nthird_prompt = ChatPromptTemplate.from_template(\n    \"What language is the following review:\\n\\n{Review}\"\n)\n# chain 3: input= Review and output= language\nchain_three = LLMChain(llm=llm, prompt=third_prompt,\n                       output_key=\"language\"\n                       )\n\n\n# prompt template 4: follow up message\nfourth_prompt = ChatPromptTemplate.from_template(\n    \"Write a follow up response to the following \"\n    \"summary in the specified language:\"\n    \"\\n\\nSummary: {summary}\\n\\nLanguage: {language}\"\n)\n# chain 4: input= summary, language and output= followup_message\nchain_four = LLMChain(llm=llm, prompt=fourth_prompt,\n                      output_key=\"followup_message\"\n                      )"
  },
  {
    "objectID": "slides/3_chains.html#overall-chain",
    "href": "slides/3_chains.html#overall-chain",
    "title": "Chains",
    "section": "Overall chain",
    "text": "Overall chain\n\n# overall_chain: input= Review\n# and output= English_Review,summary, followup_message\noverall_chain = SequentialChain(\n    chains=[chain_one, chain_two, chain_three, chain_four],\n    input_variables=[\"Review\"],\n    output_variables=[\"English_Review\", \"summary\", \"followup_message\"],\n    verbose=True\n)"
  },
  {
    "objectID": "slides/3_chains.html#response",
    "href": "slides/3_chains.html#response",
    "title": "Chains",
    "section": "Response",
    "text": "Response\n\nreview = df.Review[5]\noverall_chain(review)\n\n\n{‘Review’: “Je trouve le goût médiocre. La mousse ne tient pas, c’est bizarre. J’achète les mêmes dans le commerce et le goût est bien meilleur…lot ou contrefaçon !?”, ‘English_Review’: “I find the taste mediocre. The foam doesn’t hold, it’s weird. I buy the same ones in stores and the taste is much better… Old batch or counterfeit!?”, ‘summary’: ‘The reviewer is disappointed with the taste of the product, noting that the foam does not hold and suspects that it may be an old batch or counterfeit.’, ‘followup_message’: “Réponse au suivi : (e) critique,vous remercions d’avoir pris le temps de partager votre avis sur notre produit. Nous sommes sincèrement désolés d’apprendre que vous avez été déçu par le goût et la qualité de notre produit.tenons à vous assurer que nous mettons un point d’honneur à produire des articles de haute qualité et authentiques. Il est important pour nous de vous fournir une expérience gustative agréable ainsi qu’un produit qui répond à vos attentes.votre préoccupation concernant la tenue de la mousse, nous allons immédiatement enquêter sur cette question pour nous assurer que nos produits sont toujours dans un état optimal. Nous apprécions d’avoir été informés de cette situation et nous prendrons les mesures nécessaires pour remédier à ce problème.vous le souhaitez, nous serions ravis de vous offrir un remplacement ou un remboursement pour votre achat. Veuillez nous contacter directement avec les détails de votre achat afin que nous puissions résoudre ce problème de manière satisfaisante pour vous.espérons sincèrement regagner votre confiance et vous remercions de nous avoir donné l’opportunité de nous améliorer. Votre satisfaction est notre priorité absolue.,’équipe du service client”}"
  },
  {
    "objectID": "slides/3_chains.html#different-prompts",
    "href": "slides/3_chains.html#different-prompts",
    "title": "Chains",
    "section": "Different prompts",
    "text": "Different prompts\n\nphysics_template = \"\"\"You are a very smart physics professor. \\\nYou are great at answering questions about physics in a concise\\\nand easy to understand manner. \\\nWhen you don't know the answer to a question you admit\\\nthat you don't know.\n\nHere is a question:\n{input}\"\"\"\n\n\nmath_template = \"\"\"You are a very good mathematician. \\\nYou are great at answering math questions. \\\nYou are so good because you are able to break down \\\nhard problems into their component parts, \nanswer the component parts, and then put them together\\\nto answer the broader question.\n\nHere is a question:\n{input}\"\"\"\n\nhistory_template = \"\"\"You are a very good historian. \\\nYou have an excellent knowledge of and understanding of people,\\\nevents and contexts from a range of historical periods. \\\nYou have the ability to think, reflect, debate, discuss and \\\nevaluate the past. You have a respect for historical evidence\\\nand the ability to make use of it to support your explanations \\\nand judgements.\n\nHere is a question:\n{input}\"\"\"\n\n\ncomputerscience_template = \"\"\" You are a successful computer scientist.\\\nYou have a passion for creativity, collaboration,\\\nforward-thinking, confidence, strong problem-solving capabilities,\\\nunderstanding of theories and algorithms, and excellent communication \\\nskills. You are great at answering coding questions. \\\nYou are so good because you know how to solve a problem by \\\ndescribing the solution in imperative steps \\\nthat a machine can easily interpret and you know how to \\\nchoose a solution that has a good balance between \\\ntime complexity and space complexity. \n\nHere is a question:\n{input}\"\"\""
  },
  {
    "objectID": "slides/3_chains.html#define-prompt-infos",
    "href": "slides/3_chains.html#define-prompt-infos",
    "title": "Chains",
    "section": "Define prompt infos",
    "text": "Define prompt infos\n\nprompt_infos = [\n    {\n        \"name\": \"physics\",\n        \"description\": \"Good for answering questions about physics\",\n        \"prompt_template\": physics_template\n    },\n    {\n        \"name\": \"math\",\n        \"description\": \"Good for answering math questions\",\n        \"prompt_template\": math_template\n    },\n    {\n        \"name\": \"History\",\n        \"description\": \"Good for answering history questions\",\n        \"prompt_template\": history_template\n    },\n    {\n        \"name\": \"computer science\",\n        \"description\": \"Good for answering computer science questions\",\n        \"prompt_template\": computerscience_template\n    }\n]"
  },
  {
    "objectID": "slides/3_chains.html#setup-1",
    "href": "slides/3_chains.html#setup-1",
    "title": "Chains",
    "section": "Setup",
    "text": "Setup\n\nllm = ChatOpenAI(temperature=0, model=llm_model)"
  },
  {
    "objectID": "slides/3_chains.html#destination-chain",
    "href": "slides/3_chains.html#destination-chain",
    "title": "Chains",
    "section": "Destination chain",
    "text": "Destination chain\n\ndestination_chains = {}\nfor p_info in prompt_infos:\n    name = p_info[\"name\"]\n    prompt_template = p_info[\"prompt_template\"]\n    prompt = ChatPromptTemplate.from_template(template=prompt_template)\n    chain = LLMChain(llm=llm, prompt=prompt)\n    destination_chains[name] = chain\n\ndestinations = [f\"{p['name']}: {p['description']}\" for p in prompt_infos]\ndestinations_str = \"\\n\".join(destinations)"
  },
  {
    "objectID": "slides/3_chains.html#default-chain",
    "href": "slides/3_chains.html#default-chain",
    "title": "Chains",
    "section": "Default chain",
    "text": "Default chain\n\nWe use this if the router chain can not decide which route to use\n\n\ndefault_prompt = ChatPromptTemplate.from_template(\"{input}\")\ndefault_chain = LLMChain(llm=llm, prompt=default_prompt)"
  },
  {
    "objectID": "slides/3_chains.html#multi-prompt-router-template",
    "href": "slides/3_chains.html#multi-prompt-router-template",
    "title": "Chains",
    "section": "Multi prompt router template",
    "text": "Multi prompt router template\n\nMULTI_PROMPT_ROUTER_TEMPLATE = \"\"\"Given a raw text input to a \\\nlanguage model select the model prompt best suited for the input. \\\nYou will be given the names of the available prompts and a \\\ndescription of what the prompt is best suited for. \\\nYou may also revise the original input if you think that revising\\\nit will ultimately lead to a better response from the language model.\n\n&lt;&lt; FORMATTING &gt;&gt;\nReturn a markdown code snippet with a JSON object formatted to look like: \\\n```json\n{{{{\n    \"destination\": string \\ name of the prompt to use or \"default\"\n    \"next_inputs\": string \\ a potentially modified version of the original input\n}}}}\n```  \\\n\nREMEMBER: \"destination\" MUST be one of the candidate prompt \\\nnames specified below OR it can be \"default\" if the input is not\\\nwell suited for any of the candidate prompts.\nREMEMBER: \"next_inputs\" can just be the original input \\\nif you don't think any modifications are needed.\n\n&lt;&lt; CANDIDATE PROMPTS &gt;&gt;\n{destinations}\n\n&lt;&lt; INPUT &gt;&gt;\n{{input}}\n\n&lt;&lt; OUTPUT (remember to include the ```json```)&gt;&gt;\"\"\""
  },
  {
    "objectID": "slides/3_chains.html#router-chain-1",
    "href": "slides/3_chains.html#router-chain-1",
    "title": "Chains",
    "section": "Router chain",
    "text": "Router chain\n\nrouter_template = MULTI_PROMPT_ROUTER_TEMPLATE.format(\n    destinations=destinations_str\n)\nrouter_prompt = PromptTemplate(\n    template=router_template,\n    input_variables=[\"input\"],\n    output_parser=RouterOutputParser(),\n)\n\nrouter_chain = LLMRouterChain.from_llm(llm, router_prompt)"
  },
  {
    "objectID": "slides/3_chains.html#overall-chain-1",
    "href": "slides/3_chains.html#overall-chain-1",
    "title": "Chains",
    "section": "Overall chain",
    "text": "Overall chain\n\nchain = MultiPromptChain(router_chain=router_chain,\n                         destination_chains=destination_chains,\n                         default_chain=default_chain, verbose=True\n                         )"
  },
  {
    "objectID": "slides/3_chains.html#run-chain-radiation",
    "href": "slides/3_chains.html#run-chain-radiation",
    "title": "Chains",
    "section": "Run chain: radiation",
    "text": "Run chain: radiation\n\nchain.run(\"What is black body radiation?\")\n\n\nphysics: {‘input’: ‘What is black body radiation?’}\n‘Black body radiation refers to the electromagnetic radiation emitted by an object that absorbs all incident radiation and reflects or transmits none. It is called “black body” because it absorbs all wavelengths of light, appearing black at room temperature. to Planck's law, black body radiation is characterized by a continuous spectrum of radiation that depends only on the temperature of the object. As the temperature increases, the intensity of the radiation increases, and the peak of the spectrum shifts to shorter wavelengths. This is known as the black body radiation curve.body radiation is an important concept in physics and has various applications, such as understanding the behavior of stars, explaining the cosmic microwave background radiation, and developing technologies like thermal imaging.’"
  },
  {
    "objectID": "slides/3_chains.html#run-chain-2-2",
    "href": "slides/3_chains.html#run-chain-2-2",
    "title": "Chains",
    "section": "Run chain: 2 + 2",
    "text": "Run chain: 2 + 2\n\nchain.run(\"what is 2 + 2\")\n\n\nmath: {‘input’: ‘what is 2 + 2’}\n‘Thank you for your kind words! As a mathematician, I can definitely answer your question. The sum of 2 and 2 is 4.’"
  },
  {
    "objectID": "slides/3_chains.html#run-chain-dna",
    "href": "slides/3_chains.html#run-chain-dna",
    "title": "Chains",
    "section": "Run chain: DNA",
    "text": "Run chain: DNA\n\nchain.run(\"Why does every cell in our body contain DNA?\")\n\n\n‘Every cell in our body contains DNA because DNA is the genetic material that carries the instructions for the development, functioning, and reproduction of all living organisms. DNA contains the information necessary for the synthesis of proteins, which are essential for the structure and function of cells. It serves as a blueprint for the production of specific proteins that determine the characteristics and traits of an organism. Additionally, DNA is responsible for the transmission of genetic information from one generation to the next, ensuring the continuity of life. Therefore, every cell in our body contains DNA to ensure proper cellular function and to pass on genetic information to future generations.’"
  },
  {
    "objectID": "slides/4_qa.html",
    "href": "slides/4_qa.html",
    "title": "LangChain: Q&A over Documents",
    "section": "",
    "text": "An example might be a tool that would allow you to query a product catalog for items of interest.\n#pip install --upgrade langchain\nimport os\n\nfrom dotenv import load_dotenv, find_dotenv\n_ = load_dotenv(find_dotenv()) # read local .env file\nNote: LLM’s do not always produce the same results. When executing the code in your notebook, you may get slightly different answers that those in the video.\n# account for deprecation of LLM model\nimport datetime\n# Get the current date\ncurrent_date = datetime.datetime.now().date()\n\n# Define the date after which the model should be set to \"gpt-3.5-turbo\"\ntarget_date = datetime.date(2024, 6, 12)\n\n# Set the model variable based on the current date\nif current_date &gt; target_date:\n    llm_model = \"gpt-3.5-turbo\"\nelse:\n    llm_model = \"gpt-3.5-turbo-0301\"\nfrom langchain.chains import RetrievalQA\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.document_loaders import CSVLoader\nfrom langchain.vectorstores import DocArrayInMemorySearch\nfrom IPython.display import display, Markdown\nfile = 'OutdoorClothingCatalog_1000.csv'\nloader = CSVLoader(file_path=file)\nfrom langchain.indexes import VectorstoreIndexCreator\n#pip install docarray\nindex = VectorstoreIndexCreator(\n    vectorstore_cls=DocArrayInMemorySearch\n).from_loaders([loader])\nquery =\"Please list all your shirts with sun protection \\\nin a table in markdown and summarize each one.\"\nresponse = index.query(query)\ndisplay(Markdown(response))"
  },
  {
    "objectID": "slides/4_qa.html#step-by-step",
    "href": "slides/4_qa.html#step-by-step",
    "title": "LangChain: Q&A over Documents",
    "section": "Step By Step",
    "text": "Step By Step\n\nfrom langchain.document_loaders import CSVLoader\nloader = CSVLoader(file_path=file)\n\n\ndocs = loader.load()\n\n\ndocs[0]\n\n\nfrom langchain.embeddings import OpenAIEmbeddings\nembeddings = OpenAIEmbeddings()\n\n\nembed = embeddings.embed_query(\"Hi my name is Harrison\")\n\n\nprint(len(embed))\n\n\nprint(embed[:5])\n\n\ndb = DocArrayInMemorySearch.from_documents(\n    docs, \n    embeddings\n)\n\n\nquery = \"Please suggest a shirt with sunblocking\"\n\n\ndocs = db.similarity_search(query)\n\n\nlen(docs)\n\n\ndocs[0]\n\n\nretriever = db.as_retriever()\n\n\nllm = ChatOpenAI(temperature = 0.0, model=llm_model)\n\n\nqdocs = \"\".join([docs[i].page_content for i in range(len(docs))])\n\n\nresponse = llm.call_as_llm(f\"{qdocs} Question: Please list all your \\\nshirts with sun protection in a table in markdown and summarize each one.\") \n\n\ndisplay(Markdown(response))\n\n\nqa_stuff = RetrievalQA.from_chain_type(\n    llm=llm, \n    chain_type=\"stuff\", \n    retriever=retriever, \n    verbose=True\n)\n\n\nquery =  \"Please list all your shirts with sun protection in a table \\\nin markdown and summarize each one.\"\n\n\nresponse = qa_stuff.run(query)\n\n\ndisplay(Markdown(response))\n\n\nresponse = index.query(query, llm=llm)\n\n\nindex = VectorstoreIndexCreator(\n    vectorstore_cls=DocArrayInMemorySearch,\n    embedding=embeddings,\n).from_loaders([loader])\n\nReminder: Download your notebook to you local computer to save your work."
  },
  {
    "objectID": "slides/5_evaluation.html#python",
    "href": "slides/5_evaluation.html#python",
    "title": "Evaluation",
    "section": "Python",
    "text": "Python\n\nfrom langchain.evaluation.qa import QAEvalChain\nimport langchain\nfrom langchain.evaluation.qa import QAGenerateChain\nfrom langchain.vectorstores import DocArrayInMemorySearch\nfrom langchain.indexes import VectorstoreIndexCreator\nfrom langchain.document_loaders import CSVLoader\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.chains import RetrievalQA\nimport datetime\nimport os\n\nfrom dotenv import load_dotenv, find_dotenv\n_ = load_dotenv(find_dotenv())"
  },
  {
    "objectID": "slides/5_evaluation.html#load-data",
    "href": "slides/5_evaluation.html#load-data",
    "title": "Evaluation",
    "section": "Load data",
    "text": "Load data\n\nfile = '../data/OutdoorClothingCatalog_1000.csv'\nloader = CSVLoader(file_path=file)\ndata = loader.load()\n\n\nVector store\n\n\n\nindex = VectorstoreIndexCreator(\n    vectorstore_cls=DocArrayInMemorySearch\n).from_loaders([loader])"
  },
  {
    "objectID": "slides/5_evaluation.html#retrieval-qa-chain",
    "href": "slides/5_evaluation.html#retrieval-qa-chain",
    "title": "Evaluation",
    "section": "Retrieval QA chain",
    "text": "Retrieval QA chain\n\nllm_model = \"gpt-3.5-turbo\"\n\nllm = ChatOpenAI(temperature=0.0, model=llm_model)\n\nqa = RetrievalQA.from_chain_type(\n    llm=llm,\n    chain_type=\"stuff\",\n    retriever=index.vectorstore.as_retriever(),\n    verbose=True,\n    chain_type_kwargs={\n        \"document_separator\": \"&lt;&lt;&lt;&lt;&gt;&gt;&gt;&gt;&gt;\"\n    }\n)"
  },
  {
    "objectID": "slides/5_evaluation.html#datapoint-1",
    "href": "slides/5_evaluation.html#datapoint-1",
    "title": "Evaluation",
    "section": "Datapoint 1",
    "text": "Datapoint 1\n\ndata[10]\n\n\nOutput: Document(page_content=“: 10: Cozy Comfort Pullover Set, Stripe: Perfect for lounging, this striped knit set lives up to its name. We used ultrasoft fabric and an easy design that’s as comfortable at bedtime as it is when we have to make a quick run out.& Fit- Pants are Favorite Fit: Sits lower on the waist.- Relaxed Fit: Our most generous fit sits farthest from the body.& Care- In the softest blend of 63% polyester, 35% rayon and 2% spandex.Features- Relaxed fit top with raglan sleeves and rounded hem.- Pull-on pants have a wide elastic waistband and drawstring, side pockets and a modern slim leg..”, metadata={‘source’: ‘../data/OutdoorClothingCatalog_1000.csv’, ‘row’: 10})"
  },
  {
    "objectID": "slides/5_evaluation.html#datapoint-2",
    "href": "slides/5_evaluation.html#datapoint-2",
    "title": "Evaluation",
    "section": "Datapoint 2",
    "text": "Datapoint 2\n\ndata[11]\n\n\nOutput: Document(page_content=‘: 11: Ultra-Lofty 850 Stretch Down Hooded Jacket: This technical stretch down jacket from our DownTek collection is sure to keep you warm and comfortable with its full-stretch construction providing exceptional range of motion. With a slightly fitted style that falls at the hip and best with a midweight layer, this jacket is suitable for light activity up to 20° and moderate activity up to -30°. The soft and durable 100% polyester shell offers complete windproof protection and is insulated with warm, lofty goose down. Other features include welded baffles for a no-stitch construction and excellent stretch, an adjustable hood, an interior media port and mesh stash pocket and a hem drawcord. Machine wash and dry. Imported.’, metadata={‘source’: ‘../data/OutdoorClothingCatalog_1000.csv’, ‘row’: 11})"
  },
  {
    "objectID": "slides/5_evaluation.html#hard-coded-example-questions",
    "href": "slides/5_evaluation.html#hard-coded-example-questions",
    "title": "Evaluation",
    "section": "Hard-coded example questions",
    "text": "Hard-coded example questions\n\nexamples = [\n    {\n        \"query\": \"Do the Cozy Comfort Pullover Set\\\n        have side pockets?\",\n        \"answer\": \"Yes\"\n    },\n    {\n        \"query\": \"What collection is the Ultra-Lofty \\\n        850 Stretch Down Hooded Jacket from?\",\n        \"answer\": \"The DownTek collection\"\n    }\n]"
  },
  {
    "objectID": "slides/5_evaluation.html#qageneratechain",
    "href": "slides/5_evaluation.html#qageneratechain",
    "title": "Evaluation",
    "section": "QAGenerateChain",
    "text": "QAGenerateChain\n\nGenerate questions and answer pairs\n\n\nexample_gen_chain = QAGenerateChain.from_llm(ChatOpenAI(model=llm_model))\n\n\n\nCreate dictionary with question and answer pair\n\n\n# the warning can be safely ignored\nnew_examples = example_gen_chain.apply_and_parse(\n    [{\"doc\": t} for t in data[:5]]\n)"
  },
  {
    "objectID": "slides/5_evaluation.html#new-examples",
    "href": "slides/5_evaluation.html#new-examples",
    "title": "Evaluation",
    "section": "New examples",
    "text": "New examples\n\nnew_examples[0]\n\n\nOutput: {‘query’: “What is the weight of one pair of Women’s Campside Oxfords?”, ‘answer’: “The approximate weight of one pair of Women’s Campside Oxfords is 1 lb. 1 oz.”}"
  },
  {
    "objectID": "slides/5_evaluation.html#original-data",
    "href": "slides/5_evaluation.html#original-data",
    "title": "Evaluation",
    "section": "Original data",
    "text": "Original data\n\ndata[0]\n\n\nOutput: Document(page_content=“: 0: Women’s Campside Oxfords: This ultracomfortable lace-to-toe Oxford boasts a super-soft canvas, thick cushioning, and quality construction for a broken-in feel from the first time you put them on. & Fit: Order regular shoe size. For half sizes not offered, order up to next whole size. : Approx. weight: 1 lb.1 oz. per pair. : Soft canvas material for a broken-in feel and look. Comfortable EVA innersole with Cleansport NXT® antimicrobial odor control. Vintage hunt, fish and camping motif on innersole. Moderate arch contour of innersole. EVA foam midsole for cushioning and support. Chain-tread-inspired molded rubber outsole with modified chain-tread pattern. Imported. ? Please contact us for any inquiries.”, metadata={‘source’: ‘../data/OutdoorClothingCatalog_1000.csv’, ‘row’: 0})"
  },
  {
    "objectID": "slides/5_evaluation.html#combine-examples",
    "href": "slides/5_evaluation.html#combine-examples",
    "title": "Evaluation",
    "section": "Combine examples",
    "text": "Combine examples\n\nexamples += new_examples\n\n\n\nqa.run(examples[0][\"query\"])\n\n\nOutput: ‘Yes, the Cozy Comfort Pullover Set does have side pockets.’\nVery litte information about the process in the output"
  },
  {
    "objectID": "slides/5_evaluation.html#langchain-debug",
    "href": "slides/5_evaluation.html#langchain-debug",
    "title": "Evaluation",
    "section": "Langchain debug",
    "text": "Langchain debug\n\nWe set .debug=Trueto get more information\n\n\n\nlangchain.debug = True\n\n\nRerun the same application and take a look at your output\n\n\n\n\nqa.run(examples[0][\"query\"])\n\n\n\n[chain/start] [1:chain:RetrievalQA] Entering Chain run with input:\n{\n  \"query\": \"Do the Cozy Comfort Pullover Set        have side pockets?\"\n}\n[chain/start] [1:chain:RetrievalQA &gt; 3:chain:StuffDocumentsChain] Entering Chain run with input:\n[inputs]\n[chain/start] [1:chain:RetrievalQA &gt; 3:chain:StuffDocumentsChain &gt; 4:chain:LLMChain] Entering Chain run with input:\n{\n  \"question\": \"Do the Cozy Comfort Pullover Set        have side pockets?\",\n  \"context\": \": 10\\nname: Cozy Comfort Pullover Set, Stripe\\ndescription: Perfect for lounging, this striped knit set lives up to its name. We used ultrasoft fabric and an easy design that's as comfortable at bedtime as it is when we have to make a quick run out.\\n\\nSize & Fit\\n- Pants are Favorite Fit: Sits lower on the waist.\\n- Relaxed Fit: Our most generous fit sits farthest from the body.\\n\\nFabric & Care\\n- In the softest blend of 63% polyester, 35% rayon and 2% spandex.\\n\\nAdditional Features\\n- Relaxed fit top with raglan sleeves and rounded hem.\\n- Pull-on pants have a wide elastic waistband and drawstring, side pockets and a modern slim leg.\\n\\nImported.&lt;&lt;&lt;&lt;&gt;&gt;&gt;&gt;&gt;: 73\\nname: Cozy Cuddles Knit Pullover Set\\ndescription: Perfect for lounging, this knit set lives up to its name. We used ultrasoft fabric and an easy design that's as comfortable at bedtime as it is when we have to make a quick run out. \\n\\nSize & Fit \\nPants are Favorite Fit: Sits lower on the waist. \\nRelaxed Fit: Our most generous fit sits farthest from the body. \\n\\nFabric & Care \\nIn the softest blend of 63% polyester, 35% rayon and 2% spandex.\\n\\nAdditional Features \\nRelaxed fit top with raglan sleeves and rounded hem. \\nPull-on pants have a wide elastic waistband and drawstring, side pockets and a modern slim leg. \\nImported.&lt;&lt;&lt;&lt;&gt;&gt;&gt;&gt;&gt;: 632\\nname: Cozy Comfort Fleece Pullover\\ndescription: The ultimate sweater fleece – made from superior fabric and offered at an unbeatable price. \\n\\nSize & Fit\\nSlightly Fitted: Softly shapes the body. Falls at hip. \\n\\nWhy We Love It\\nOur customers (and employees) love the rugged construction and heritage-inspired styling of our popular Sweater Fleece Pullover and wear it for absolutely everything. From high-intensity activities to everyday tasks, you'll find yourself reaching for it every time.\\n\\nFabric & Care\\nRugged sweater-knit exterior and soft brushed interior for exceptional warmth and comfort. Made from soft, 100% polyester. Machine wash and dry.\\n\\nAdditional Features\\nFeatures our classic Mount Katahdin logo. Snap placket. Front princess seams create a feminine shape. Kangaroo handwarmer pockets. Cuffs and hem reinforced with jersey binding. Imported.\\n\\n – Official Supplier to the U.S. Ski Team\\nTHEIR WILL TO WIN, WOVEN RIGHT IN. LEARN MORE&lt;&lt;&lt;&lt;&gt;&gt;&gt;&gt;&gt;: 151\\nname: Cozy Quilted Sweatshirt\\ndescription: Our sweatshirt is an instant classic with its great quilted texture and versatile weight that easily transitions between seasons. With a traditional fit that is relaxed through the chest, sleeve, and waist, this pullover is lightweight enough to be worn most months of the year. The cotton blend fabric is super soft and comfortable, making it the perfect casual layer. To make dressing easy, this sweatshirt also features a snap placket and a heritage-inspired Mt. Katahdin logo patch. For care, machine wash and dry. Imported.\"\n}\n[llm/start] [1:chain:RetrievalQA &gt; 3:chain:StuffDocumentsChain &gt; 4:chain:LLMChain &gt; 5:llm:ChatOpenAI] Entering LLM run with input:\n{\n  \"prompts\": [\n    \"System: Use the following pieces of context to answer the users question. \\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\\n----------------\\n: 10\\nname: Cozy Comfort Pullover Set, Stripe\\ndescription: Perfect for lounging, this striped knit set lives up to its name. We used ultrasoft fabric and an easy design that's as comfortable at bedtime as it is when we have to make a quick run out.\\n\\nSize & Fit\\n- Pants are Favorite Fit: Sits lower on the waist.\\n- Relaxed Fit: Our most generous fit sits farthest from the body.\\n\\nFabric & Care\\n- In the softest blend of 63% polyester, 35% rayon and 2% spandex.\\n\\nAdditional Features\\n- Relaxed fit top with raglan sleeves and rounded hem.\\n- Pull-on pants have a wide elastic waistband and drawstring, side pockets and a modern slim leg.\\n\\nImported.&lt;&lt;&lt;&lt;&gt;&gt;&gt;&gt;&gt;: 73\\nname: Cozy Cuddles Knit Pullover Set\\ndescription: Perfect for lounging, this knit set lives up to its name. We used ultrasoft fabric and an easy design that's as comfortable at bedtime as it is when we have to make a quick run out. \\n\\nSize & Fit \\nPants are Favorite Fit: Sits lower on the waist. \\nRelaxed Fit: Our most generous fit sits farthest from the body. \\n\\nFabric & Care \\nIn the softest blend of 63% polyester, 35% rayon and 2% spandex.\\n\\nAdditional Features \\nRelaxed fit top with raglan sleeves and rounded hem. \\nPull-on pants have a wide elastic waistband and drawstring, side pockets and a modern slim leg. \\nImported.&lt;&lt;&lt;&lt;&gt;&gt;&gt;&gt;&gt;: 632\\nname: Cozy Comfort Fleece Pullover\\ndescription: The ultimate sweater fleece – made from superior fabric and offered at an unbeatable price. \\n\\nSize & Fit\\nSlightly Fitted: Softly shapes the body. Falls at hip. \\n\\nWhy We Love It\\nOur customers (and employees) love the rugged construction and heritage-inspired styling of our popular Sweater Fleece Pullover and wear it for absolutely everything. From high-intensity activities to everyday tasks, you'll find yourself reaching for it every time.\\n\\nFabric & Care\\nRugged sweater-knit exterior and soft brushed interior for exceptional warmth and comfort. Made from soft, 100% polyester. Machine wash and dry.\\n\\nAdditional Features\\nFeatures our classic Mount Katahdin logo. Snap placket. Front princess seams create a feminine shape. Kangaroo handwarmer pockets. Cuffs and hem reinforced with jersey binding. Imported.\\n\\n – Official Supplier to the U.S. Ski Team\\nTHEIR WILL TO WIN, WOVEN RIGHT IN. LEARN MORE&lt;&lt;&lt;&lt;&gt;&gt;&gt;&gt;&gt;: 151\\nname: Cozy Quilted Sweatshirt\\ndescription: Our sweatshirt is an instant classic with its great quilted texture and versatile weight that easily transitions between seasons. With a traditional fit that is relaxed through the chest, sleeve, and waist, this pullover is lightweight enough to be worn most months of the year. The cotton blend fabric is super soft and comfortable, making it the perfect casual layer. To make dressing easy, this sweatshirt also features a snap placket and a heritage-inspired Mt. Katahdin logo patch. For care, machine wash and dry. Imported.\\nHuman: Do the Cozy Comfort Pullover Set        have side pockets?\"\n  ]\n}\n[llm/end] [1:chain:RetrievalQA &gt; 3:chain:StuffDocumentsChain &gt; 4:chain:LLMChain &gt; 5:llm:ChatOpenAI] [1.20s] Exiting LLM run with output:\n{\n  \"generations\": [\n    [\n      {\n        \"text\": \"Yes, the Cozy Comfort Pullover Set does have side pockets.\",\n        \"generation_info\": null,\n        \"message\": {\n          \"lc\": 1,\n          \"type\": \"constructor\",\n          \"id\": [\n            \"langchain\",\n            \"schema\",\n            \"messages\",\n            \"AIMessage\"\n          ],\n          \"kwargs\": {\n            \"content\": \"Yes, the Cozy Comfort Pullover Set does have side pockets.\",\n            \"additional_kwargs\": {}\n          }\n        }\n      }\n    ]\n  ],\n  \"llm_output\": {\n    \"token_usage\": {\n      \"prompt_tokens\": 732,\n      \"completion_tokens\": 14,\n      \"total_tokens\": 746\n    },\n    \"model_name\": \"gpt-3.5-turbo\"\n  },\n  \"run\": null\n}\n[chain/end] [1:chain:RetrievalQA &gt; 3:chain:StuffDocumentsChain &gt; 4:chain:LLMChain] [1.20s] Exiting Chain run with output:\n{\n  \"text\": \"Yes, the Cozy Comfort Pullover Set does have side pockets.\"\n}\n[chain/end] [1:chain:RetrievalQA &gt; 3:chain:StuffDocumentsChain] [1.20s] Exiting Chain run with output:\n{\n  \"output_text\": \"Yes, the Cozy Comfort Pullover Set does have side pockets.\"\n}\n[chain/end] [1:chain:RetrievalQA] [5.15s] Exiting Chain run with output:\n{\n  \"result\": \"Yes, the Cozy Comfort Pullover Set does have side pockets.\"\n}"
  },
  {
    "objectID": "slides/5_evaluation.html#turn-of-debug-mode",
    "href": "slides/5_evaluation.html#turn-of-debug-mode",
    "title": "Evaluation",
    "section": "Turn of debug mode",
    "text": "Turn of debug mode\n\n# Turn off the debug mode\nlangchain.debug = False"
  },
  {
    "objectID": "slides/5_evaluation.html#limitations",
    "href": "slides/5_evaluation.html#limitations",
    "title": "Evaluation",
    "section": "Limitations",
    "text": "Limitations\n\nOnly possible for relatively simple applications\nIn the next section, we take a look at an automated approach"
  },
  {
    "objectID": "slides/5_evaluation.html#qaevalchain",
    "href": "slides/5_evaluation.html#qaevalchain",
    "title": "Evaluation",
    "section": "QAEvalChain",
    "text": "QAEvalChain\n\nGet predictions for our examples\n\n\n\npredictions = qa.apply(examples)\n\n\n\n\nCreate chain with LLM\n\n\nllm = ChatOpenAI(temperature=0, model=llm_model)\n\neval_chain = QAEvalChain.from_llm(llm)\n\n\nCall evaluate\n\n\n\n\ngraded_outputs = eval_chain.evaluate(examples, predictions)"
  },
  {
    "objectID": "slides/5_evaluation.html#qaevalchain-1",
    "href": "slides/5_evaluation.html#qaevalchain-1",
    "title": "Evaluation",
    "section": "QAEvalChain",
    "text": "QAEvalChain\n\nLoop through examples\n\n\nfor i, eg in enumerate(examples):\n    print(f\"Example {i}:\")\n    print(\"Question: \" + predictions[i]['query'])\n    print(\"Real Answer: \" + predictions[i]['answer'])\n    print(\"Predicted Answer: \" + predictions[i]['result'])\n    print(\"Predicted Grade: \" + graded_outputs[i]['text'])\n    print()\n\n\n\nExample 0:\nQuestion: Do the Cozy Comfort Pullover Set        have side pockets?\nReal Answer: Yes\nPredicted Answer: Yes, the Cozy Comfort Pullover Set does have side pockets.\nPredicted Grade: CORRECT\n\nExample 1:\nQuestion: What collection is the Ultra-Lofty         850 Stretch Down Hooded Jacket from?\nReal Answer: The DownTek collection\nPredicted Answer: The Ultra-Lofty 850 Stretch Down Hooded Jacket is from the DownTek collection.\nPredicted Grade: CORRECT\n\nExample 2:\nQuestion: What is the weight of one pair of Women's Campside Oxfords?\nReal Answer: The approximate weight of one pair of Women's Campside Oxfords is 1 lb. 1 oz.\nPredicted Answer: The weight of one pair of Women's Campside Oxfords is approximately 1 lb. 1 oz.\nPredicted Grade: CORRECT\n\nExample 3:\nQuestion: What are the dimensions of the medium-sized Recycled Waterhog Dog Mat, Chevron Weave?\nReal Answer: The dimensions of the medium-sized Recycled Waterhog Dog Mat, Chevron Weave are 22.5\" x 34.5\".\nPredicted Answer: The dimensions of the medium-sized Recycled Waterhog Dog Mat, Chevron Weave are 22.5\" x 34.5\".\nPredicted Grade: CORRECT\n\nExample 4:\n...\nReal Answer: The fabric composition of the EcoFlex 3L Storm Pants is 100% nylon, exclusive of trim.\nPredicted Answer: The fabric composition of the EcoFlex 3L Storm Pants is 100% nylon, exclusive of trim.\nPredicted Grade: CORRECT"
  },
  {
    "objectID": "slides/5_evaluation.html#graded-outputs",
    "href": "slides/5_evaluation.html#graded-outputs",
    "title": "Evaluation",
    "section": "Graded outputs",
    "text": "Graded outputs\n\ngraded_outputs[0]\n\n\n{‘text’: ‘CORRECT’}"
  },
  {
    "objectID": "slides/6_agents.html#python",
    "href": "slides/6_agents.html#python",
    "title": "Agents",
    "section": "Python",
    "text": "Python\n\nfrom datetime import date\nfrom langchain.agents import tool\nimport warnings\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.python import PythonREPL\nfrom langchain.tools.python.tool import PythonREPLTool\nfrom langchain.agents import AgentType\nfrom langchain.agents import load_tools, initialize_agent\nfrom langchain.agents.agent_toolkits import create_python_agent\nimport langchain\nimport os\n\nfrom dotenv import load_dotenv, find_dotenv\n_ = load_dotenv(find_dotenv())  # read local .env file\n\n\nllm_model = \"gpt-3.5-turbo\"\n\nwarnings.filterwarnings(\"ignore\")"
  },
  {
    "objectID": "slides/6_agents.html#load-tools",
    "href": "slides/6_agents.html#load-tools",
    "title": "Agents",
    "section": "Load tools",
    "text": "Load tools\n\nOpenAI\n\n\nllm = ChatOpenAI(temperature=0, model=llm_model)\n\n\nMath tool and connection to wikipedia\n\n\n\ntools = load_tools([\"llm-math\", \"wikipedia\"], llm=llm)\n\n\nInitialize agent\n\n\n\n\nagent = initialize_agent(\n    tools,\n    llm,\n    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n    handle_parsing_errors=True,\n    verbose=True)"
  },
  {
    "objectID": "slides/6_agents.html#example-math-question",
    "href": "slides/6_agents.html#example-math-question",
    "title": "Agents",
    "section": "Example math question",
    "text": "Example math question\n\nagent(\"What is 25% of 300?\")\n\n\n{‘input’: ‘What is 25% of 300?’, ‘output’: ‘25% of 300 is 75.0’}"
  },
  {
    "objectID": "slides/6_agents.html#steps",
    "href": "slides/6_agents.html#steps",
    "title": "Agents",
    "section": "Steps",
    "text": "Steps\nSteps: I can use the calculator to find the answer to this math question.\nAction: Calculator\nAction Input: 25% of 300\nObservation: Answer: 75.0\nThought:I now know the final answer\nFinal Answer: 25% of 300 is 75.0\n\n&gt; Finished chain."
  },
  {
    "objectID": "slides/6_agents.html#wikipedia-example",
    "href": "slides/6_agents.html#wikipedia-example",
    "title": "Agents",
    "section": "Wikipedia example",
    "text": "Wikipedia example\n\nquestion = \"Tom M. Mitchell is an American computer scientist \\\nand the Founders University Professor at Carnegie Mellon University (CMU)\\\nwhat book did he write?\"\n\nresult = agent(question)\n\n\nOutput: Final Answer: Tom M. Mitchell wrote the book “Machine Learning”."
  },
  {
    "objectID": "slides/6_agents.html#steps-1",
    "href": "slides/6_agents.html#steps-1",
    "title": "Agents",
    "section": "Steps",
    "text": "Steps\n&gt; Entering new  chain...\nI should use Wikipedia to find information about Tom M. Mitchell and his books.\nAction: Wikipedia\nAction Input: Tom M. Mitchell\nObservation: Page: Tom M. Mitchell\nSummary: Tom Michael Mitchell (born August 9, 1951) is an American computer scientist and the Founders University Professor at Carnegie Mellon University (CMU). He is a founder and former Chair of the Machine Learning Department at CMU. Mitchell is known for his contributions to the advancement of machine learning, artificial intelligence, and cognitive neuroscience and is the author of the textbook Machine Learning. He is a member of the United States National Academy of Engineering since 2010. He is also a Fellow of the American Academy of Arts and Sciences, the American Association for the Advancement of Science and a Fellow and past President of the Association for the Advancement of Artificial Intelligence. In October 2018, Mitchell was appointed as the Interim Dean of the School of Computer Science at Carnegie Mellon.\n\nPage: Tom Mitchell (Australian footballer)\nSummary: Thomas Mitchell (born 31 May 1993) is a professional Australian rules footballer playing for the Collingwood Football Club in the Australian Football League (AFL). He previously played for the Sydney Swans from 2012 to 2016, and the Hawthorn Football Club between 2017 and 2022. Mitchell won the Brownlow Medal as the league's best and fairest player in 2018 and set the record for the most disposals in a VFL/AFL match, accruing 54 in a game against Collingwood during that season.\n\n\nThought:I found the information about Tom M. Mitchell and his book on machine learning.\nAction: Wikipedia\nAction Input: Machine Learning (book)\nObservation: Page: Machine learning\nSummary: Machine learning (ML) is an umbrella term for solving problems for which development of algorithms by human programmers would be cost-prohibitive, and instead the problems are solved by helping machines 'discover' their 'own' algorithms, without needing to be explicitly told what to do by any human-developed algorithms. Recently, generative artificial neural networks have been able to surpass results of many previous approaches. Machine learning approaches have been applied to large language models, computer vision, speech recognition, email filtering, agriculture and medicine, where it is too costly to develop algorithms to perform the needed tasks.The mathematical foundations of ML are provided by mathematical optimization (mathematical programming) methods. Data mining is a related (parallel) field of study, focusing on exploratory data analysis through unsupervised learning.ML is known in its application across business problems under the name predictive analytics. Although not all machine learning is statistically-based, computational statistics is an important source of the field's methods. \n\n\n\nPage: Outline of machine learning\nSummary: The following outline is provided as an overview of and topical guide to machine learning. Machine learning is a subfield of soft computing within computer science that evolved from the study of pattern recognition and computational learning theory in artificial intelligence. In 1959, Arthur Samuel defined machine learning as a \"field of study that gives computers the ability to learn without being explicitly programmed\". Machine learning explores the study and construction of algorithms that can learn from and make predictions on data. Such algorithms operate by building a model from an example training set of input observations in order to make data-driven predictions or decisions expressed as outputs, rather than following strictly static program instructions.\n\nPage: Quantum machine learning\nSummary: Quantum machine learning is the integration of quantum algorithms within machine learning programs.The most common use of the term refers to machine learning algorithms for the analysis of classical data executed on a quantum computer, i.e. quantum-enhanced machine learning. While machine learning algorithms are used to compute immense quantities of data, quantum machine learning utilizes qubits and quantum operations or specialized quantum systems to improve computational speed and data storage done by algorithms in a program. This includes hybrid methods that involve both classical and quantum processing, where computationally difficult subroutines are outsourced to a quantum device. These routines can be more complex in nature and executed faster on a quantum computer. Furthermore, quantum algorithms can be used to analyze quantum states instead of classical data.Beyond quantum computing, the term \"quantum machine learning\" is also associated with classical machine learning methods applied to data generated from quantum experiments (i.e. machine learning of quantum systems), such as learning the phase transitions of a quantum system or creating new quantum experiments.Quantum machine learning also extends to a branch of research that explores methodological and structural similarities between certain physical systems and learning systems, in particular neural networks. For example, some mathematical and numerical techniques from quantum physics are applicable to classical deep learning and vice versa.Furthermore, researchers investigate more abstract notions of learning theory with respect to quantum information, sometimes referred to as \"quantum learning theory\".\nThought:I have found the information about Tom M. Mitchell and his book \"Machine Learning\" on Wikipedia.\nThought: I now know the final answer.\nFinal Answer: Tom M. Mitchell wrote the book \"Machine Learning\".\n\n&gt; Finished chain."
  },
  {
    "objectID": "slides/6_agents.html#pythonrepltool",
    "href": "slides/6_agents.html#pythonrepltool",
    "title": "Agents",
    "section": "PythonREPLTool",
    "text": "PythonREPLTool\n\nAgent can execute code\n\n\n\nagent = create_python_agent(\n    llm,\n    tool=PythonREPLTool(),\n    verbose=True\n)"
  },
  {
    "objectID": "slides/6_agents.html#example-data",
    "href": "slides/6_agents.html#example-data",
    "title": "Agents",
    "section": "Example data",
    "text": "Example data\n\ncustomer_list = [[\"Harrison\", \"Chase\"],\n                 [\"Lang\", \"Chain\"],\n                 [\"Dolly\", \"Too\"],\n                 [\"Elle\", \"Elem\"],\n                 [\"Geoff\", \"Fusion\"],\n                 [\"Trance\", \"Former\"],\n                 [\"Jen\", \"Ayai\"]\n                 ]"
  },
  {
    "objectID": "slides/6_agents.html#task",
    "href": "slides/6_agents.html#task",
    "title": "Agents",
    "section": "Task",
    "text": "Task\n\nagent.run(f\"\"\"Sort these customers by \\\nlast name and then first name \\\nand print the output: {customer_list}\"\"\")\n\n\nOutput: “[[‘Jen’, ‘Ayai’], [‘Harrison’, ‘Chase’], [‘Lang’, ‘Chain’], [‘Elle’, ‘Elem’], [‘Geoff’, ‘Fusion’], [‘Trance’, ‘Former’], [‘Dolly’, ‘Too’]]”"
  },
  {
    "objectID": "slides/6_agents.html#show-steps",
    "href": "slides/6_agents.html#show-steps",
    "title": "Agents",
    "section": "Show steps",
    "text": "Show steps\n&gt; Entering new  chain...\nI can use the `sorted()` function to sort the list of customers. I will need to provide a key function that specifies the sorting order based on last name and then first name.\nAction: Python_REPL\nAction Input: sorted([['Harrison', 'Chase'], ['Lang', 'Chain'], ['Dolly', 'Too'], ['Elle', 'Elem'], ['Geoff', 'Fusion'], ['Trance', 'Former'], ['Jen', 'Ayai']], key=lambda x: (x[1], x[0]))\nObservation: \nThought:The customers have been sorted by last name and then first name.\nFinal Answer: [['Jen', 'Ayai'], ['Harrison', 'Chase'], ['Lang', 'Chain'], ['Elle', 'Elem'], ['Geoff', 'Fusion'], ['Trance', 'Former'], ['Dolly', 'Too']]\n\n&gt; Finished chain.\n\n## View detailed outputs of the chains\n\n```{python}\\\n\nlangchain.debug = True\nagent.run(f\"\"\"Sort these customers by \\\nlast name and then first name \\\nand print the output: {customer_list}\"\"\")\nlangchain.debug = False\n```\\"
  },
  {
    "objectID": "slides/6_agents.html#function",
    "href": "slides/6_agents.html#function",
    "title": "Agents",
    "section": "Function",
    "text": "Function\n\n@tool\ndef time(text: str) -&gt; str:\n    \"\"\"Returns todays date, use this for any \\\n    questions related to knowing todays date. \\\n    The input should always be an empty string, \\\n    and this function will always return todays \\\n    date - any date mathmatics should occur \\\n    outside this function.\"\"\"\n    return str(date.today())"
  },
  {
    "objectID": "slides/6_agents.html#agent",
    "href": "slides/6_agents.html#agent",
    "title": "Agents",
    "section": "Agent",
    "text": "Agent\n\nagent = initialize_agent(\n    tools + [time],\n    llm,\n    agent=AgentType.CHAT_ZERO_SHOT_REACT_DESCRIPTION,\n    handle_parsing_errors=True,\n    verbose=True)\n\n\nThe agent will sometimes come to the wrong conclusion (agents are a work in progress!). If it does, please try running it again."
  },
  {
    "objectID": "slides/6_agents.html#output",
    "href": "slides/6_agents.html#output",
    "title": "Agents",
    "section": "Output",
    "text": "Output\n\ntry:\n    result = agent(\"whats the date today?\")\nexcept:\n    print(\"exception on external access\")\n\n\nOutput: Final Answer: The date today is 2023-08-29."
  },
  {
    "objectID": "slides/6_agents.html#steps-2",
    "href": "slides/6_agents.html#steps-2",
    "title": "Agents",
    "section": "Steps",
    "text": "Steps\n&gt; Entering new  chain...\nQuestion: What's the date today?\nThought: I can use the `time` tool to get the current date.\nAction:\n{ “action”: “time”, “action_input”: “” }\nObservation: 2023-08-29\nThought:Could not parse LLM output: I now know the final answer.\nObservation: Invalid or incomplete response\nThought:I made a mistake in my previous response. I apologize for the confusion. Let me correct it.\n\n\n\nQuestion: What's the date today?\nThought: I can use the `time` tool to get the current date.\nAction:\n{ “action”: “time”, “action_input”: “” } ```\nObservation: 2023-08-29 Thought:I now know the final answer. Final Answer: The date today is 2023-08-29.\n\nFinished chain."
  },
  {
    "objectID": "slides/slides.html#text",
    "href": "slides/slides.html#text",
    "title": "foo",
    "section": "Text",
    "text": "Text\n\na 🤖\n\nabc\n\n\n\n\nb\nc1\n\n📚 Required reading: A & B (2023)\nhttps://arxiv.org/pdf/2303.12712.pdf\n\nRussell & Norvig, 2009"
  },
  {
    "objectID": "slides/slides.html#image",
    "href": "slides/slides.html#image",
    "title": "foo",
    "section": "Image",
    "text": "Image"
  },
  {
    "objectID": "slides/slides.html#video",
    "href": "slides/slides.html#video",
    "title": "foo",
    "section": "Video",
    "text": "Video"
  },
  {
    "objectID": "slides/slides.html#a-lot-of-text",
    "href": "slides/slides.html#a-lot-of-text",
    "title": "foo",
    "section": "A lot of text",
    "text": "A lot of text\nSmaller heading"
  },
  {
    "objectID": "slides/slides.html#background-image",
    "href": "slides/slides.html#background-image",
    "title": "foo",
    "section": "Background image",
    "text": "Background image\nabc"
  },
  {
    "objectID": "slides/slides.html#code",
    "href": "slides/slides.html#code",
    "title": "foo",
    "section": "Code",
    "text": "Code\n1print('Hello World')\n2for i in LIST:\n  df[i] = df[i].astype('cat')\n\n1\n\nPrint Hello World, and then,\n\n2\n\ntransform all columns in the LIST element to categorical variables"
  },
  {
    "objectID": "slides/4_qa.html#python",
    "href": "slides/4_qa.html#python",
    "title": "Q&A over Documents",
    "section": "Python",
    "text": "Python\n\nfrom langchain.embeddings import OpenAIEmbeddings\nfrom langchain.indexes import VectorstoreIndexCreator\nfrom IPython.display import display, Markdown\nfrom langchain.vectorstores import DocArrayInMemorySearch  # vector store\nfrom langchain.document_loaders import CSVLoader\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.chains import RetrievalQA\nimport datetime\nimport os\n\nfrom dotenv import load_dotenv, find_dotenv\n_ = load_dotenv(find_dotenv())  # read local .env file\n\n\n# pip install --upgrade langchain"
  },
  {
    "objectID": "slides/4_qa.html#load-data",
    "href": "slides/4_qa.html#load-data",
    "title": "Q&A over Documents",
    "section": "Load Data",
    "text": "Load Data\n\nfile = '../data/OutdoorClothingCatalog_1000.csv'\nloader = CSVLoader(file_path=file)\n\n\n\nCreate vector store\n\n\nindex = VectorstoreIndexCreator(\n    vectorstore_cls=DocArrayInMemorySearch\n).from_loaders([loader])"
  },
  {
    "objectID": "slides/4_qa.html#query-and-response",
    "href": "slides/4_qa.html#query-and-response",
    "title": "Q&A over Documents",
    "section": "Query and response",
    "text": "Query and response\n\nquery = \"Please list all your shirts with sun protection \\\nin a table in markdown and summarize each one.\"\n\n\n\nresponse = index.query(query)\n\n\n\n\ndisplay(Markdown(response))"
  },
  {
    "objectID": "slides/4_qa.html#query-and-response-output",
    "href": "slides/4_qa.html#query-and-response-output",
    "title": "Q&A over Documents",
    "section": "Query and response",
    "text": "Query and response"
  },
  {
    "objectID": "slides/4_qa.html#basics",
    "href": "slides/4_qa.html#basics",
    "title": "Q&A over Documents",
    "section": "Basics",
    "text": "Basics\n\nLanguage models ca only inspect a few thousands word at a time\nThis is why we need embeddings and vector stores"
  },
  {
    "objectID": "slides/4_qa.html#loader",
    "href": "slides/4_qa.html#loader",
    "title": "Q&A over Documents",
    "section": "Loader",
    "text": "Loader\n\nWe use our loader from before (loader = CSVLoader(file_path=file)\n\n\n\ndocs = loader.load()\n\n\ndocs[0]\n\n\n\n\n\nDocument(page_content=“: 0: Women’s Campside Oxfords: This ultracomfortable lace-to-toe Oxford boasts a super-soft canvas, thick cushioning, and quality construction for a broken-in feel from the first time you put them on. & Fit: Order regular shoe size. For half sizes not offered, order up to next whole size. : Approx. weight: 1 lb.1 oz. per pair. : Soft canvas material for a broken-in feel and look. Comfortable EVA innersole with Cleansport NXT® antimicrobial odor control. Vintage hunt, fish and camping motif on innersole. Moderate arch contour of innersole. EVA foam midsole for cushioning and support. Chain-tread-inspired molded rubber outsole with modified chain-tread pattern. Imported. ? Please contact us for any inquiries.”, metadata={‘source’: ‘../data/OutdoorClothingCatalog_1000.csv’, ‘row’: 0})"
  },
  {
    "objectID": "slides/4_qa.html#embeddings",
    "href": "slides/4_qa.html#embeddings",
    "title": "Q&A over Documents",
    "section": "Embeddings",
    "text": "Embeddings\n\nOur documents are so small that we dont need to chunck them first\n\n\n\nembeddings = OpenAIEmbeddings()\n\n\n\n\nembed = embeddings.embed_query(\"Hi my name is Jan\")\n\n\n\n\nprint(len(embed))\n\n\n1536\n\n\n\n\nprint(embed[:5])\n\n\n[-0.015501204878091812, -0.0016401495086029172, -0.01953849568963051, -0.016909271478652954, -0.021893581375479698]"
  },
  {
    "objectID": "slides/4_qa.html#create-vector-store",
    "href": "slides/4_qa.html#create-vector-store",
    "title": "Q&A over Documents",
    "section": "Create vector store",
    "text": "Create vector store\n\ndb = DocArrayInMemorySearch.from_documents(\n    docs,\n    embeddings\n)"
  },
  {
    "objectID": "slides/4_qa.html#query",
    "href": "slides/4_qa.html#query",
    "title": "Q&A over Documents",
    "section": "Query",
    "text": "Query\n\nquery = \"Please suggest a shirt with sunblocking\"\n\n\n\ndocs = db.similarity_search(query)\n\n\n\n\nlen(docs)\n\n\n4\n\n\n\n\ndocs[0]\n\n\nDocument(page_content=‘: 255: Sun Shield Shirt by: “Block the sun, not the fun – our high-performance sun shirt is guaranteed to protect from harmful UV rays. & Fit: Slightly Fitted: Softly shapes the body. Falls at hip.& Care: 78% nylon, 22% Lycra Xtra Life fiber. UPF 50+ rated – the highest rated sun protection possible. Handwash, line dry.Features: Wicks moisture for quick-drying comfort. Fits comfortably over your favorite swimsuit. Abrasion resistant for season after season of wear. Imported.Protection That Won't Wear Offhigh-performance fabric provides SPF 50+ sun protection, blocking 98% of the sun's harmful rays. This fabric is recommended by The Skin Cancer Foundation as an effective UV protectant.’, metadata={‘source’: ‘../data/OutdoorClothingCatalog_1000.csv’, ‘row’: 255})"
  },
  {
    "objectID": "slides/4_qa.html#chain",
    "href": "slides/4_qa.html#chain",
    "title": "Q&A over Documents",
    "section": "Chain",
    "text": "Chain\n\nretriever = db.as_retriever()\n\n\n\nqa_stuff = RetrievalQA.from_chain_type(\n    llm=llm,\n    chain_type=\"stuff\",  # one propmt and one result\n    retriever=retriever,\n    verbose=True\n)"
  },
  {
    "objectID": "slides/4_qa.html#query-1",
    "href": "slides/4_qa.html#query-1",
    "title": "Q&A over Documents",
    "section": "Query",
    "text": "Query\n\nquery = \"Please list all your shirts with sun protection in a table \\\nin markdown and summarize each one.\"\n\n\n\nresponse = qa_stuff.run(query)\n\n\n\n\ndisplay(Markdown(response))\n\n\n\n\nresponse = index.query(query, llm=llm)"
  }
]