{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Chains in LangChain\n",
                "\n",
                "LLMChain, Sequential Chains, SimpleSequentialChain, SequentialChain & Router Chain\n",
                "\n",
                "# Setup\n",
                "\n",
                "## Python"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "from langchain.chains.router.llm_router import LLMRouterChain, RouterOutputParser\n",
                "from langchain.prompts import PromptTemplate\n",
                "from langchain.chains.router import MultiPromptChain\n",
                "from langchain.chains import SequentialChain\n",
                "from langchain.chains import SimpleSequentialChain\n",
                "from langchain.chains import LLMChain\n",
                "from langchain.prompts import ChatPromptTemplate\n",
                "from langchain.chat_models import ChatOpenAI\n",
                "from dotenv import load_dotenv, find_dotenv\n",
                "import os\n",
                "import pandas as pd\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "_ = load_dotenv(find_dotenv())  # read local .env file"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Data"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "df = pd.read_csv('../data/Data.csv')"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "df.head()"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# LLM Chain\n",
                "\n",
                "\n",
                "## LLMChain {.smaller}\n",
                "\n",
                "\n",
                "- Initialize model\n",
                "\n",
                ""
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "llm_model = \"gpt-3.5-turbo\"\n",
                "\n",
                "llm = ChatOpenAI(temperature=0.9, model=llm_model)"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "- Initialize prompt\n",
                "\n",
                ""
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "prompt = ChatPromptTemplate.from_template(\n",
                "    \"What is the best name to describe \\\n",
                "    a company that makes {product}?\"\n",
                ")"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "\n",
                "- Combine to chain"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "chain = LLMChain(llm=llm, prompt=prompt)"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Example"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "product = \"Queen Size Sheet Set\"\n",
                "\n",
                "chain.run(product)"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "- '\"Royal Rest Linens\"'\n",
                "\n",
                "# Simple Sequential Chain\n",
                "\n",
                "Works well with one single input and one single output\n",
                "\n",
                "## SimpleSequentialChain {.smaller}\n"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "llm = ChatOpenAI(temperature=0.9, model=llm_model)\n",
                "\n",
                "# prompt template 1\n",
                "first_prompt = ChatPromptTemplate.from_template(\n",
                "    \"What is the best name to describe \\\n",
                "    a company that makes {product}?\"\n",
                ")\n",
                "\n",
                "# Chain 1\n",
                "chain_one = LLMChain(llm=llm, prompt=first_prompt)"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                ""
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "# prompt template 2\n",
                "second_prompt = ChatPromptTemplate.from_template(\n",
                "    \"Write a 20 words description for the following \\\n",
                "    company:{company_name}\"\n",
                ")\n",
                "# chain 2\n",
                "chain_two = LLMChain(llm=llm, prompt=second_prompt)"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Overall simple chain"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "overall_simple_chain = SimpleSequentialChain(chains=[chain_one, chain_two],\n",
                "                                             verbose=True\n",
                "                                             )"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                ""
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "overall_simple_chain.run(product)"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "- 'Regal Rest Linens offers high-quality, luxurious linens for a comfortable and rejuvenating sleep experience. Perfect for a regal touch.'\n",
                "\n",
                "# Sequential Chain\n",
                "\n",
                "Multiple inputs and/or outputs\n",
                "\n",
                "## SequentialChain {.smaller}\n"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "llm = ChatOpenAI(temperature=0.9, model=llm_model)\n",
                "\n",
                "# prompt template 1: translate to english\n",
                "first_prompt = ChatPromptTemplate.from_template(\n",
                "    \"Translate the following review to english:\"\n",
                "    \"\\n\\n{Review}\"\n",
                ")\n",
                "# chain 1: input= Review and output= English_Review\n",
                "chain_one = LLMChain(llm=llm, prompt=first_prompt,\n",
                "                     output_key=\"English_Review\"\n",
                "                     )"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                ""
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "second_prompt = ChatPromptTemplate.from_template(\n",
                "    \"Can you summarize the following review in 1 sentence:\"\n",
                "    \"\\n\\n{English_Review}\"\n",
                ")\n",
                "# chain 2: input= English_Review and output= summary\n",
                "chain_two = LLMChain(llm=llm, prompt=second_prompt,\n",
                "                     output_key=\"summary\"\n",
                "                     )"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## "
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "# prompt template 3: translate to english\n",
                "third_prompt = ChatPromptTemplate.from_template(\n",
                "    \"What language is the following review:\\n\\n{Review}\"\n",
                ")\n",
                "# chain 3: input= Review and output= language\n",
                "chain_three = LLMChain(llm=llm, prompt=third_prompt,\n",
                "                       output_key=\"language\"\n",
                "                       )"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "# prompt template 4: follow up message\n",
                "fourth_prompt = ChatPromptTemplate.from_template(\n",
                "    \"Write a follow up response to the following \"\n",
                "    \"summary in the specified language:\"\n",
                "    \"\\n\\nSummary: {summary}\\n\\nLanguage: {language}\"\n",
                ")\n",
                "# chain 4: input= summary, language and output= followup_message\n",
                "chain_four = LLMChain(llm=llm, prompt=fourth_prompt,\n",
                "                      output_key=\"followup_message\"\n",
                "                      )"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Overall chain"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "# overall_chain: input= Review\n",
                "# and output= English_Review,summary, followup_message\n",
                "overall_chain = SequentialChain(\n",
                "    chains=[chain_one, chain_two, chain_three, chain_four],\n",
                "    input_variables=[\"Review\"],\n",
                "    output_variables=[\"English_Review\", \"summary\", \"followup_message\"],\n",
                "    verbose=True\n",
                ")"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Response {.samller}"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "review = df.Review[5]\n",
                "overall_chain(review)"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "- {'Review': \"Je trouve le goût médiocre. La mousse ne tient pas, c'est bizarre. J'achète les mêmes dans le commerce et le goût est bien meilleur...\\nVieux lot ou contrefaçon !?\",\n",
                " 'English_Review': \"I find the taste mediocre. The foam doesn't hold, it's weird. I buy the same ones in stores and the taste is much better... Old batch or counterfeit!?\",\n",
                " 'summary': 'The reviewer is disappointed with the taste of the product, noting that the foam does not hold and suspects that it may be an old batch or counterfeit.',\n",
                " 'followup_message': \"Réponse au suivi : \\n\\nCher(e) critique,\\n\\nNous vous remercions d'avoir pris le temps de partager votre avis sur notre produit. Nous sommes sincèrement désolés d'apprendre que vous avez été déçu par le goût et la qualité de notre produit.\\n\\nNous tenons à vous assurer que nous mettons un point d'honneur à produire des articles de haute qualité et authentiques. Il est important pour nous de vous fournir une expérience gustative agréable ainsi qu'un produit qui répond à vos attentes.\\n\\nConcernant votre préoccupation concernant la tenue de la mousse, nous allons immédiatement enquêter sur cette question pour nous assurer que nos produits sont toujours dans un état optimal. Nous apprécions d'avoir été informés de cette situation et nous prendrons les mesures nécessaires pour remédier à ce problème.\\n\\nSi vous le souhaitez, nous serions ravis de vous offrir un remplacement ou un remboursement pour votre achat. Veuillez nous contacter directement avec les détails de votre achat afin que nous puissions résoudre ce problème de manière satisfaisante pour vous.\\n\\nNous espérons sincèrement regagner votre confiance et vous remercions de nous avoir donné l'opportunité de nous améliorer. Votre satisfaction est notre priorité absolue.\\n\\nCordialement,\\n\\nL'équipe du service client\"}\n",
                "\n",
                "# Router Chain\n",
                "\n",
                "Specialized subchains for different types of input \n",
                "\n",
                "## Different prompts {.smaller}"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "physics_template = \"\"\"You are a very smart physics professor. \\\n",
                "You are great at answering questions about physics in a concise\\\n",
                "and easy to understand manner. \\\n",
                "When you don't know the answer to a question you admit\\\n",
                "that you don't know.\n",
                "\n",
                "Here is a question:\n",
                "{input}\"\"\"\n",
                "\n",
                "\n",
                "math_template = \"\"\"You are a very good mathematician. \\\n",
                "You are great at answering math questions. \\\n",
                "You are so good because you are able to break down \\\n",
                "hard problems into their component parts, \n",
                "answer the component parts, and then put them together\\\n",
                "to answer the broader question.\n",
                "\n",
                "Here is a question:\n",
                "{input}\"\"\"\n",
                "\n",
                "history_template = \"\"\"You are a very good historian. \\\n",
                "You have an excellent knowledge of and understanding of people,\\\n",
                "events and contexts from a range of historical periods. \\\n",
                "You have the ability to think, reflect, debate, discuss and \\\n",
                "evaluate the past. You have a respect for historical evidence\\\n",
                "and the ability to make use of it to support your explanations \\\n",
                "and judgements.\n",
                "\n",
                "Here is a question:\n",
                "{input}\"\"\"\n",
                "\n",
                "\n",
                "computerscience_template = \"\"\" You are a successful computer scientist.\\\n",
                "You have a passion for creativity, collaboration,\\\n",
                "forward-thinking, confidence, strong problem-solving capabilities,\\\n",
                "understanding of theories and algorithms, and excellent communication \\\n",
                "skills. You are great at answering coding questions. \\\n",
                "You are so good because you know how to solve a problem by \\\n",
                "describing the solution in imperative steps \\\n",
                "that a machine can easily interpret and you know how to \\\n",
                "choose a solution that has a good balance between \\\n",
                "time complexity and space complexity. \n",
                "\n",
                "Here is a question:\n",
                "{input}\"\"\""
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Define prompt infos"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "prompt_infos = [\n",
                "    {\n",
                "        \"name\": \"physics\",\n",
                "        \"description\": \"Good for answering questions about physics\",\n",
                "        \"prompt_template\": physics_template\n",
                "    },\n",
                "    {\n",
                "        \"name\": \"math\",\n",
                "        \"description\": \"Good for answering math questions\",\n",
                "        \"prompt_template\": math_template\n",
                "    },\n",
                "    {\n",
                "        \"name\": \"History\",\n",
                "        \"description\": \"Good for answering history questions\",\n",
                "        \"prompt_template\": history_template\n",
                "    },\n",
                "    {\n",
                "        \"name\": \"computer science\",\n",
                "        \"description\": \"Good for answering computer science questions\",\n",
                "        \"prompt_template\": computerscience_template\n",
                "    }\n",
                "]"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Setup"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "llm = ChatOpenAI(temperature=0, model=llm_model)"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Destination chain"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "destination_chains = {}\n",
                "for p_info in prompt_infos:\n",
                "    name = p_info[\"name\"]\n",
                "    prompt_template = p_info[\"prompt_template\"]\n",
                "    prompt = ChatPromptTemplate.from_template(template=prompt_template)\n",
                "    chain = LLMChain(llm=llm, prompt=prompt)\n",
                "    destination_chains[name] = chain\n",
                "\n",
                "destinations = [f\"{p['name']}: {p['description']}\" for p in prompt_infos]\n",
                "destinations_str = \"\\n\".join(destinations)"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Default chain\n",
                "\n",
                "- We use this if the router chain can not decide which route to use"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "default_prompt = ChatPromptTemplate.from_template(\"{input}\")\n",
                "default_chain = LLMChain(llm=llm, prompt=default_prompt)"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Multi prompt router template {.smaller}"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "MULTI_PROMPT_ROUTER_TEMPLATE = \"\"\"Given a raw text input to a \\\n",
                "language model select the model prompt best suited for the input. \\\n",
                "You will be given the names of the available prompts and a \\\n",
                "description of what the prompt is best suited for. \\\n",
                "You may also revise the original input if you think that revising\\\n",
                "it will ultimately lead to a better response from the language model.\n",
                "\n",
                "<< FORMATTING >>\n",
                "Return a markdown code snippet with a JSON object formatted to look like: \\\n",
                "```json\n",
                "{{{{\n",
                "    \"destination\": string \\ name of the prompt to use or \"default\"\n",
                "    \"next_inputs\": string \\ a potentially modified version of the original input\n",
                "}}}}\n",
                "```  \\\n",
                "\n",
                "REMEMBER: \"destination\" MUST be one of the candidate prompt \\\n",
                "names specified below OR it can be \"default\" if the input is not\\\n",
                "well suited for any of the candidate prompts.\n",
                "REMEMBER: \"next_inputs\" can just be the original input \\\n",
                "if you don't think any modifications are needed.\n",
                "\n",
                "<< CANDIDATE PROMPTS >>\n",
                "{destinations}\n",
                "\n",
                "<< INPUT >>\n",
                "{{input}}\n",
                "\n",
                "<< OUTPUT (remember to include the ```json```)>>\"\"\""
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Router chain"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "router_template = MULTI_PROMPT_ROUTER_TEMPLATE.format(\n",
                "    destinations=destinations_str\n",
                ")\n",
                "router_prompt = PromptTemplate(\n",
                "    template=router_template,\n",
                "    input_variables=[\"input\"],\n",
                "    output_parser=RouterOutputParser(),\n",
                ")\n",
                "\n",
                "router_chain = LLMRouterChain.from_llm(llm, router_prompt)"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Overall chain"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "chain = MultiPromptChain(router_chain=router_chain,\n",
                "                         destination_chains=destination_chains,\n",
                "                         default_chain=default_chain, verbose=True\n",
                "                         )"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Run chain: radiation"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "chain.run(\"What is black body radiation?\")"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "- physics: {'input': 'What is black body radiation?'}\n",
                "- 'Black body radiation refers to the electromagnetic radiation emitted by an object that absorbs all incident radiation and reflects or transmits none. It is called \"black body\" because it absorbs all wavelengths of light, appearing black at room temperature. \\n\\nAccording to Planck\\'s law, black body radiation is characterized by a continuous spectrum of radiation that depends only on the temperature of the object. As the temperature increases, the intensity of the radiation increases, and the peak of the spectrum shifts to shorter wavelengths. This is known as the black body radiation curve.\\n\\nBlack body radiation is an important concept in physics and has various applications, such as understanding the behavior of stars, explaining the cosmic microwave background radiation, and developing technologies like thermal imaging.'\n",
                "\n",
                "\n",
                "## Run chain: 2 + 2"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "chain.run(\"what is 2 + 2\")"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "- math: {'input': 'what is 2 + 2'}\n",
                "\n",
                "- 'Thank you for your kind words! As a mathematician, I can definitely answer your question. The sum of 2 and 2 is 4.'\n",
                "\n",
                "## Run chain: DNA\n"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "chain.run(\"Why does every cell in our body contain DNA?\")"
            ],
            "execution_count": null,
            "outputs": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "name": "python3",
            "language": "python",
            "display_name": "Python 3 (ipykernel)"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}