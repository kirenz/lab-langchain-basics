{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Memory\n",
                "\n",
                "ConversationBufferMemory\n",
                "\n",
                "ConversationBufferWindowMemory\n",
                "\n",
                "ConversationTokenBufferMemory\n",
                "\n",
                "ConversationSummaryMemory\n",
                "\n",
                "# Setup\n",
                "\n",
                "## Python"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "from langchain.memory import ConversationSummaryBufferMemory\n",
                "from langchain.llms import OpenAI\n",
                "from langchain.memory import ConversationTokenBufferMemory\n",
                "from langchain.memory import ConversationBufferWindowMemory\n",
                "from langchain.memory import ConversationBufferMemory\n",
                "from langchain.chains import ConversationChain\n",
                "from langchain.chat_models import ChatOpenAI\n",
                "import datetime\n",
                "import warnings\n",
                "import os\n",
                "\n",
                "from dotenv import load_dotenv, find_dotenv\n",
                "_ = load_dotenv(find_dotenv())  # read local .env file\n",
                "\n",
                "warnings.filterwarnings('ignore')"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Conversation Buffer Memory\n",
                "\n",
                "## Setup"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "llm_model = \"gpt-3.5-turbo\"\n",
                "\n",
                "llm = ChatOpenAI(temperature=0.0, model=llm_model)\n",
                "memory = ConversationBufferMemory()\n",
                "\n",
                "conversation = ConversationChain(\n",
                "    llm=llm,\n",
                "    memory=memory,\n",
                "    verbose=True  # shows conversation history\n",
                ")"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Some examples"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "conversation.predict(input=\"Hi, my name is Jan\")"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "- \"Hello Jan! It's nice to meet you. How can I assist you today?\""
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "conversation.predict(input=\"What is 1+1?\")"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "- '1+1 is equal to 2.'"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "conversation.predict(input=\"What is my name?\")"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "- 'Your name is Jan.'\n",
                "\n",
                "## Memory buffer\n"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "print(memory.buffer)"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "\n",
                "```markdown\n",
                "Human: Hi, my name is Jan\n",
                "AI: Hello Jan! It's nice to meet you. How can I assist you today?\n",
                "Human: What is 1+1?\n",
                "AI: 1+1 is equal to 2.\n",
                "Human: What is my name?\n",
                "AI: Your name is Jan.\n",
                "```\n",
                ""
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "memory.load_memory_variables({})"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "- {'history': \"Human: Hi, my name is Jan\\nAI: Hello Jan! It's nice to meet you. How can I assist you today?\\nHuman: What is 1+1?\\nAI: 1+1 is equal to 2.\\nHuman: What is my name?\\nAI: Your name is Jan.\"}\n",
                "\n",
                "\n",
                "## Conversational example"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "memory = ConversationBufferMemory()"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "memory.save_context({\"input\": \"Hi\"},\n",
                "                    {\"output\": \"What's up\"})"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                ""
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "print(memory.buffer)"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "\n",
                "```markdown\n",
                "Human: Hi\n",
                "AI: What's up\n",
                "```\n",
                "\n",
                ""
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "memory.load_memory_variables({})"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "- {'history': \"Human: Hi\\nAI: What's up\"}\n",
                "\n",
                "## Conversational example continued\n"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "memory.save_context({\"input\": \"Not much, just hanging\"},\n",
                "                    {\"output\": \"Cool\"})"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                ""
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "memory.load_memory_variables({})"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "- {'history': \"Human: Hi\\nAI: What's up\\nHuman: Not much, just hanging\\nAI: Cool\"}\n",
                "\n",
                "# Conversation Buffer Window Memory\n",
                "\n",
                "## Conversation Buffer Window Memory"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "memory = ConversationBufferWindowMemory(k=1)"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Example"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "memory.save_context({\"input\": \"Hi\"},\n",
                "                    {\"output\": \"What's up\"})\n",
                "\n",
                "memory.save_context({\"input\": \"Not much, just hanging\"},\n",
                "                    {\"output\": \"Cool\"})"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                ""
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "memory.load_memory_variables({})"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "- {'history': 'Human: Not much, just hanging\\nAI: Cool'}\n",
                "\n",
                "## Setup"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "llm = ChatOpenAI(temperature=0.0, model=llm_model)\n",
                "\n",
                "memory = ConversationBufferWindowMemory(k=1)\n",
                "\n",
                "conversation = ConversationChain(\n",
                "    llm=llm,\n",
                "    memory=memory,\n",
                "    verbose=False\n",
                ")"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Example {.smaller}"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "conversation.predict(input=\"Hi, my name is Jan\")"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "- \"Hello Jan! It's nice to meet you. How can I assist you today?\"\n",
                "\n",
                ""
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "conversation.predict(input=\"What is 1+1?\")"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "- '1+1 is equal to 2.'\n",
                "\n",
                ""
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "conversation.predict(input=\"What is my name?\")"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "- \"I'm sorry, but I don't have access to personal information about individuals unless it has been shared with me in the course of our conversation.\"\n",
                "\n",
                "\n",
                "# Conversation TokenBuffer Memory\n",
                "\n",
                "## Setup"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "llm = ChatOpenAI(temperature=0.0, model=llm_model)"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "memory = ConversationTokenBufferMemory(llm=llm, max_token_limit=50)"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Example"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "memory.save_context({\"input\": \"AI is what?!\"},\n",
                "                    {\"output\": \"Amazing!\"})\n",
                "\n",
                "memory.save_context({\"input\": \"Backpropagation is what?\"},\n",
                "                    {\"output\": \"Beautiful!\"})\n",
                "memory.save_context({\"input\": \"Chatbots are what?\"},\n",
                "                    {\"output\": \"Charming!\"})"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                ""
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "memory.load_memory_variables({})"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "- {'history': 'AI: Amazing!\\nHuman: Backpropagation is what?\\nAI: Beautiful!\\nHuman: Chatbots are what?\\nAI: Charming!'}\n",
                "\n",
                "# Conversation Summary Memory\n",
                "\n",
                "## Example"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "# create a long string\n",
                "schedule = \"There is a meeting at 8am with your product team. \\\n",
                "You will need your powerpoint presentation prepared. \\\n",
                "9am-12pm have time to work on your LangChain \\\n",
                "project which will go quickly because Langchain is such a powerful tool. \\\n",
                "At Noon, lunch at the italian resturant with a customer who is driving \\\n",
                "from over an hour away to meet you to understand the latest in AI. \\\n",
                "Be sure to bring your laptop to show the latest LLM demo.\""
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Setup"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "memory = ConversationSummaryBufferMemory(llm=llm, max_token_limit=100)"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Example"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "memory.save_context({\"input\": \"Hello\"}, {\"output\": \"What's up\"})\n",
                "memory.save_context({\"input\": \"Not much, just hanging\"},\n",
                "                    {\"output\": \"Cool\"})\n",
                "memory.save_context({\"input\": \"What is on the schedule today?\"},\n",
                "                    {\"output\": f\"{schedule}\"})"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## History"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "memory.load_memory_variables({})"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "- {'history': 'System: The human and AI exchange greetings. The human asks about the schedule for the day. The AI provides a detailed schedule, including a meeting with the product team, work on the LangChain project, and a lunch meeting with a customer interested in AI. The AI emphasizes the importance of bringing a laptop to showcase the latest LLM demo during the lunch meeting.'}\n",
                "\n",
                "## ConversationChain \n"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "conversation = ConversationChain(\n",
                "    llm=llm,\n",
                "    memory=memory,\n",
                "    verbose=True\n",
                ")"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Conversation predict {.smaller}"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "conversation.predict(input=\"What would be a good demo to show?\")"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "- \"A good demo to show during the lunch meeting with the customer interested in AI would be the latest LLM (Language Model) demo. The LLM is a cutting-edge AI model that can generate human-like text based on a given prompt. It has been trained on a vast amount of data and can generate coherent and contextually relevant responses. By showcasing the LLM demo, you can demonstrate the capabilities of AI in natural language processing and generate interest in potential applications for the customer's business.\"\n",
                "\n",
                "\n",
                "## Memory {.smaller}\n"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "memory.load_memory_variables({})"
            ],
            "execution_count": null,
            "outputs": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "name": "python3",
            "language": "python",
            "display_name": "Python 3 (ipykernel)"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}