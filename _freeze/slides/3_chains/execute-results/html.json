{
  "hash": "522ef9f8bc90a8f651d2d321a06de543",
  "result": {
    "markdown": "---\ntitle: Chains\ntitle-slide-attributes:\n  data-background-image: ../images/logo.png\n  data-background-size: contain\n  data-background-opacity: \"0.5\"\nlang: en\nsubtitle: LangChain Basics 3\nauthor: Jan Kirenz\nexecute:\n  eval: false\n  echo: true\nhighlight-style: github\nformat:\n  revealjs: \n    toc: true\n    toc-depth: 1\n    embed-resources: false\n    theme: [dark, ../custom.scss]  \n    incremental: true\n    transition: slide\n    background-transition: fade\n    transition-speed: slow\n    code-copy: true\n    code-line-numbers: true\n    smaller: false\n    scrollable: true\n    slide-number: c\n    preview-links: auto\n    chalkboard: \n      buttons: false\n   # logo: ../images/logo.png\n    footer: Jan Kirenz\n---\n\n# Chains in LangChain\n\nLLMChain, Sequential Chains, SimpleSequentialChain, SequentialChain & Router Chain\n\n# Setup\n\n## Python\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nfrom langchain.chains.router.llm_router import LLMRouterChain, RouterOutputParser\nfrom langchain.prompts import PromptTemplate\nfrom langchain.chains.router import MultiPromptChain\nfrom langchain.chains import SequentialChain\nfrom langchain.chains import SimpleSequentialChain\nfrom langchain.chains import LLMChain\nfrom langchain.prompts import ChatPromptTemplate\nfrom langchain.chat_models import ChatOpenAI\nfrom dotenv import load_dotenv, find_dotenv\nimport os\nimport pandas as pd\nimport warnings\nwarnings.filterwarnings('ignore')\n\n_ = load_dotenv(find_dotenv())  # read local .env file\n```\n:::\n\n\n## Data\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\ndf = pd.read_csv('../data/Data.csv')\n```\n:::\n\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\ndf.head()\n```\n:::\n\n\n# LLM Chain\n\n\n## LLMChain {.smaller}\n\n\n- Initialize model\n\n. . .\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\nllm_model = \"gpt-3.5-turbo\"\n\nllm = ChatOpenAI(temperature=0.9, model=llm_model)\n```\n:::\n\n\n- Initialize prompt\n\n. . .\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\nprompt = ChatPromptTemplate.from_template(\n    \"What is the best name to describe \\\n    a company that makes {product}?\"\n)\n```\n:::\n\n\n. . .\n\n- Combine to chain\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\nchain = LLMChain(llm=llm, prompt=prompt)\n```\n:::\n\n\n## Example\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\nproduct = \"Queen Size Sheet Set\"\n\nchain.run(product)\n```\n:::\n\n\n- '\"Royal Rest Linens\"'\n\n# Simple Sequential Chain\n\nWorks well with one single input and one single output\n\n## SimpleSequentialChain {.smaller}\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\nllm = ChatOpenAI(temperature=0.9, model=llm_model)\n\n# prompt template 1\nfirst_prompt = ChatPromptTemplate.from_template(\n    \"What is the best name to describe \\\n    a company that makes {product}?\"\n)\n\n# Chain 1\nchain_one = LLMChain(llm=llm, prompt=first_prompt)\n```\n:::\n\n\n. . .\n\n::: {.cell execution_count=9}\n``` {.python .cell-code}\n# prompt template 2\nsecond_prompt = ChatPromptTemplate.from_template(\n    \"Write a 20 words description for the following \\\n    company:{company_name}\"\n)\n# chain 2\nchain_two = LLMChain(llm=llm, prompt=second_prompt)\n```\n:::\n\n\n## Overall simple chain\n\n::: {.cell execution_count=10}\n``` {.python .cell-code}\noverall_simple_chain = SimpleSequentialChain(chains=[chain_one, chain_two],\n                                             verbose=True\n                                             )\n```\n:::\n\n\n. . .\n\n::: {.cell execution_count=11}\n``` {.python .cell-code}\noverall_simple_chain.run(product)\n```\n:::\n\n\n- 'Regal Rest Linens offers high-quality, luxurious linens for a comfortable and rejuvenating sleep experience. Perfect for a regal touch.'\n\n# Sequential Chain\n\nMultiple inputs and/or outputs\n\n## SequentialChain {.smaller}\n\n::: {.cell execution_count=12}\n``` {.python .cell-code}\nllm = ChatOpenAI(temperature=0.9, model=llm_model)\n\n# prompt template 1: translate to english\nfirst_prompt = ChatPromptTemplate.from_template(\n    \"Translate the following review to english:\"\n    \"\\n\\n{Review}\"\n)\n# chain 1: input= Review and output= English_Review\nchain_one = LLMChain(llm=llm, prompt=first_prompt,\n                     output_key=\"English_Review\"\n                     )\n```\n:::\n\n\n. . .\n\n::: {.cell execution_count=13}\n``` {.python .cell-code}\nsecond_prompt = ChatPromptTemplate.from_template(\n    \"Can you summarize the following review in 1 sentence:\"\n    \"\\n\\n{English_Review}\"\n)\n# chain 2: input= English_Review and output= summary\nchain_two = LLMChain(llm=llm, prompt=second_prompt,\n                     output_key=\"summary\"\n                     )\n```\n:::\n\n\n## \n\n::: {.cell execution_count=14}\n``` {.python .cell-code}\n# prompt template 3: translate to english\nthird_prompt = ChatPromptTemplate.from_template(\n    \"What language is the following review:\\n\\n{Review}\"\n)\n# chain 3: input= Review and output= language\nchain_three = LLMChain(llm=llm, prompt=third_prompt,\n                       output_key=\"language\"\n                       )\n```\n:::\n\n\n::: {.cell execution_count=15}\n``` {.python .cell-code}\n# prompt template 4: follow up message\nfourth_prompt = ChatPromptTemplate.from_template(\n    \"Write a follow up response to the following \"\n    \"summary in the specified language:\"\n    \"\\n\\nSummary: {summary}\\n\\nLanguage: {language}\"\n)\n# chain 4: input= summary, language and output= followup_message\nchain_four = LLMChain(llm=llm, prompt=fourth_prompt,\n                      output_key=\"followup_message\"\n                      )\n```\n:::\n\n\n## Overall chain\n\n::: {.cell execution_count=16}\n``` {.python .cell-code}\n# overall_chain: input= Review\n# and output= English_Review,summary, followup_message\noverall_chain = SequentialChain(\n    chains=[chain_one, chain_two, chain_three, chain_four],\n    input_variables=[\"Review\"],\n    output_variables=[\"English_Review\", \"summary\", \"followup_message\"],\n    verbose=True\n)\n```\n:::\n\n\n## Response {.samller}\n\n::: {.cell execution_count=17}\n``` {.python .cell-code}\nreview = df.Review[5]\noverall_chain(review)\n```\n:::\n\n\n- {'Review': \"Je trouve le goût médiocre. La mousse ne tient pas, c'est bizarre. J'achète les mêmes dans le commerce et le goût est bien meilleur...\\nVieux lot ou contrefaçon !?\",\n 'English_Review': \"I find the taste mediocre. The foam doesn't hold, it's weird. I buy the same ones in stores and the taste is much better... Old batch or counterfeit!?\",\n 'summary': 'The reviewer is disappointed with the taste of the product, noting that the foam does not hold and suspects that it may be an old batch or counterfeit.',\n 'followup_message': \"Réponse au suivi : \\n\\nCher(e) critique,\\n\\nNous vous remercions d'avoir pris le temps de partager votre avis sur notre produit. Nous sommes sincèrement désolés d'apprendre que vous avez été déçu par le goût et la qualité de notre produit.\\n\\nNous tenons à vous assurer que nous mettons un point d'honneur à produire des articles de haute qualité et authentiques. Il est important pour nous de vous fournir une expérience gustative agréable ainsi qu'un produit qui répond à vos attentes.\\n\\nConcernant votre préoccupation concernant la tenue de la mousse, nous allons immédiatement enquêter sur cette question pour nous assurer que nos produits sont toujours dans un état optimal. Nous apprécions d'avoir été informés de cette situation et nous prendrons les mesures nécessaires pour remédier à ce problème.\\n\\nSi vous le souhaitez, nous serions ravis de vous offrir un remplacement ou un remboursement pour votre achat. Veuillez nous contacter directement avec les détails de votre achat afin que nous puissions résoudre ce problème de manière satisfaisante pour vous.\\n\\nNous espérons sincèrement regagner votre confiance et vous remercions de nous avoir donné l'opportunité de nous améliorer. Votre satisfaction est notre priorité absolue.\\n\\nCordialement,\\n\\nL'équipe du service client\"}\n\n# Router Chain\n\nSpecialized subchains for different types of input \n\n## Different prompts {.smaller}\n\n::: {.cell execution_count=18}\n``` {.python .cell-code}\nphysics_template = \"\"\"You are a very smart physics professor. \\\nYou are great at answering questions about physics in a concise\\\nand easy to understand manner. \\\nWhen you don't know the answer to a question you admit\\\nthat you don't know.\n\nHere is a question:\n{input}\"\"\"\n\n\nmath_template = \"\"\"You are a very good mathematician. \\\nYou are great at answering math questions. \\\nYou are so good because you are able to break down \\\nhard problems into their component parts, \nanswer the component parts, and then put them together\\\nto answer the broader question.\n\nHere is a question:\n{input}\"\"\"\n\nhistory_template = \"\"\"You are a very good historian. \\\nYou have an excellent knowledge of and understanding of people,\\\nevents and contexts from a range of historical periods. \\\nYou have the ability to think, reflect, debate, discuss and \\\nevaluate the past. You have a respect for historical evidence\\\nand the ability to make use of it to support your explanations \\\nand judgements.\n\nHere is a question:\n{input}\"\"\"\n\n\ncomputerscience_template = \"\"\" You are a successful computer scientist.\\\nYou have a passion for creativity, collaboration,\\\nforward-thinking, confidence, strong problem-solving capabilities,\\\nunderstanding of theories and algorithms, and excellent communication \\\nskills. You are great at answering coding questions. \\\nYou are so good because you know how to solve a problem by \\\ndescribing the solution in imperative steps \\\nthat a machine can easily interpret and you know how to \\\nchoose a solution that has a good balance between \\\ntime complexity and space complexity. \n\nHere is a question:\n{input}\"\"\"\n```\n:::\n\n\n## Define prompt infos\n\n::: {.cell execution_count=19}\n``` {.python .cell-code}\nprompt_infos = [\n    {\n        \"name\": \"physics\",\n        \"description\": \"Good for answering questions about physics\",\n        \"prompt_template\": physics_template\n    },\n    {\n        \"name\": \"math\",\n        \"description\": \"Good for answering math questions\",\n        \"prompt_template\": math_template\n    },\n    {\n        \"name\": \"History\",\n        \"description\": \"Good for answering history questions\",\n        \"prompt_template\": history_template\n    },\n    {\n        \"name\": \"computer science\",\n        \"description\": \"Good for answering computer science questions\",\n        \"prompt_template\": computerscience_template\n    }\n]\n```\n:::\n\n\n## Setup\n\n::: {.cell execution_count=20}\n``` {.python .cell-code}\nllm = ChatOpenAI(temperature=0, model=llm_model)\n```\n:::\n\n\n## Destination chain\n\n::: {.cell execution_count=21}\n``` {.python .cell-code}\ndestination_chains = {}\nfor p_info in prompt_infos:\n    name = p_info[\"name\"]\n    prompt_template = p_info[\"prompt_template\"]\n    prompt = ChatPromptTemplate.from_template(template=prompt_template)\n    chain = LLMChain(llm=llm, prompt=prompt)\n    destination_chains[name] = chain\n\ndestinations = [f\"{p['name']}: {p['description']}\" for p in prompt_infos]\ndestinations_str = \"\\n\".join(destinations)\n```\n:::\n\n\n## Default chain\n\n- We use this if the router chain can not decide which route to use\n\n::: {.cell execution_count=22}\n``` {.python .cell-code}\ndefault_prompt = ChatPromptTemplate.from_template(\"{input}\")\ndefault_chain = LLMChain(llm=llm, prompt=default_prompt)\n```\n:::\n\n\n## Multi prompt router template {.smaller}\n\n::: {.cell execution_count=23}\n```` {.python .cell-code}\nMULTI_PROMPT_ROUTER_TEMPLATE = \"\"\"Given a raw text input to a \\\nlanguage model select the model prompt best suited for the input. \\\nYou will be given the names of the available prompts and a \\\ndescription of what the prompt is best suited for. \\\nYou may also revise the original input if you think that revising\\\nit will ultimately lead to a better response from the language model.\n\n<< FORMATTING >>\nReturn a markdown code snippet with a JSON object formatted to look like: \\\n```json\n{{{{\n    \"destination\": string \\ name of the prompt to use or \"default\"\n    \"next_inputs\": string \\ a potentially modified version of the original input\n}}}}\n```  \\\n\nREMEMBER: \"destination\" MUST be one of the candidate prompt \\\nnames specified below OR it can be \"default\" if the input is not\\\nwell suited for any of the candidate prompts.\nREMEMBER: \"next_inputs\" can just be the original input \\\nif you don't think any modifications are needed.\n\n<< CANDIDATE PROMPTS >>\n{destinations}\n\n<< INPUT >>\n{{input}}\n\n<< OUTPUT (remember to include the ```json```)>>\"\"\"\n````\n:::\n\n\n## Router chain\n\n::: {.cell execution_count=24}\n``` {.python .cell-code}\nrouter_template = MULTI_PROMPT_ROUTER_TEMPLATE.format(\n    destinations=destinations_str\n)\nrouter_prompt = PromptTemplate(\n    template=router_template,\n    input_variables=[\"input\"],\n    output_parser=RouterOutputParser(),\n)\n\nrouter_chain = LLMRouterChain.from_llm(llm, router_prompt)\n```\n:::\n\n\n## Overall chain\n\n::: {.cell execution_count=25}\n``` {.python .cell-code}\nchain = MultiPromptChain(router_chain=router_chain,\n                         destination_chains=destination_chains,\n                         default_chain=default_chain, verbose=True\n                         )\n```\n:::\n\n\n## Run chain: radiation\n\n::: {.cell execution_count=26}\n``` {.python .cell-code}\nchain.run(\"What is black body radiation?\")\n```\n:::\n\n\n- physics: {'input': 'What is black body radiation?'}\n- 'Black body radiation refers to the electromagnetic radiation emitted by an object that absorbs all incident radiation and reflects or transmits none. It is called \"black body\" because it absorbs all wavelengths of light, appearing black at room temperature. \\n\\nAccording to Planck\\'s law, black body radiation is characterized by a continuous spectrum of radiation that depends only on the temperature of the object. As the temperature increases, the intensity of the radiation increases, and the peak of the spectrum shifts to shorter wavelengths. This is known as the black body radiation curve.\\n\\nBlack body radiation is an important concept in physics and has various applications, such as understanding the behavior of stars, explaining the cosmic microwave background radiation, and developing technologies like thermal imaging.'\n\n\n## Run chain: 2 + 2\n\n::: {.cell execution_count=27}\n``` {.python .cell-code}\nchain.run(\"what is 2 + 2\")\n```\n:::\n\n\n- math: {'input': 'what is 2 + 2'}\n\n- 'Thank you for your kind words! As a mathematician, I can definitely answer your question. The sum of 2 and 2 is 4.'\n\n## Run chain: DNA\n\n::: {.cell execution_count=28}\n``` {.python .cell-code}\nchain.run(\"Why does every cell in our body contain DNA?\")\n```\n:::\n\n\n- 'Every cell in our body contains DNA because DNA is the genetic material that carries the instructions for the development, functioning, and reproduction of all living organisms. DNA contains the information necessary for the synthesis of proteins, which are essential for the structure and function of cells. It serves as a blueprint for the production of specific proteins that determine the characteristics and traits of an organism. Additionally, DNA is responsible for the transmission of genetic information from one generation to the next, ensuring the continuity of life. Therefore, every cell in our body contains DNA to ensure proper cellular function and to pass on genetic information to future generations.'\n\n\n# Acknowledgments\n\n*This tutorial is mainly based on the excellent course [\"LangChain for LLM Application Development\"](https://www.deeplearning.ai/short-courses/langchain-for-llm-application-development/) provided by Harrison Chase and Andrew Ng*\n\n# What's next? {background-image=\"../images/logo.png\" background-opacity=\"0.5\"}\n\n**Congratulations! You have completed this tutorial** 👍\n\n**Next, you may want to go back to the [lab's website](https://kirenz.github.io/langchain-basics/)**\n\n",
    "supporting": [
      "3_chains_files"
    ],
    "filters": [],
    "includes": {}
  }
}